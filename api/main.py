#!/usr/bin/env python3
"""
YT2 API ì„œë²„
ìˆ˜ì›ì‹œ í–‰ê¶ë™ YouTube ë°ì´í„° ê²€ìƒ‰ API
"""

import os
import sys
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
import psycopg2
import psycopg2.extras
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from opensearchpy import OpenSearch
import redis
from dotenv import load_dotenv

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# from sentence_transformers import SentenceTransformer  # ì˜ì¡´ì„± ë¬¸ì œë¡œ ì„ì‹œ ë¹„í™œì„±í™”

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="YT2 API",
    description="ìˆ˜ì›ì‹œ í–‰ê¶ë™ YouTube ë°ì´í„° ê²€ìƒ‰ API",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': os.getenv("DB_HOST", "localhost"),
    'port': int(os.getenv("DB_PORT", "5432")),
    'dbname': os.getenv("DB_NAME", "yt2"),
    'user': os.getenv("DB_USER", "app"),
    'password': os.getenv("DB_PASSWORD", "app1234"),
}

# OpenSearch í´ë¼ì´ì–¸íŠ¸
OS_CLIENT = OpenSearch(
    hosts=[os.getenv("OS_HOST", "http://localhost:9200")],
    http_auth=(os.getenv("OS_USER", "admin"), os.getenv("OS_PASSWORD", "App1234!@#")),
    use_ssl=False,
    verify_certs=False
)

# Redis í´ë¼ì´ì–¸íŠ¸
REDIS_CLIENT = redis.Redis(
    host=os.getenv("REDIS_HOST", "localhost"),
    port=int(os.getenv("REDIS_PORT", "6379")),
    decode_responses=True
)

# Pydantic ëª¨ë¸
class VideoResponse(BaseModel):
    id: str
    title: str
    description: Optional[str]
    published_at: Optional[str]
    channel_name: str
    view_count: int
    like_count: int
    comment_count: int
    tags: List[str]
    thumbnails: Dict
    # ì¶”ê°€ëœ í•„ë“œë“¤
    privacy_status: Optional[str]
    license: Optional[str]
    embeddable: Optional[bool]
    made_for_kids: Optional[bool]
    recording_location: Optional[Dict]
    recording_date: Optional[str]
    localizations: Optional[Dict]
    topic_categories: List[str]
    relevant_topic_ids: List[str]

class SearchResponse(BaseModel):
    videos: List[VideoResponse]
    total_count: int
    total_pages: int
    query: str
    search_time: float

class StatsResponse(BaseModel):
    total_channels: int
    total_videos: int
    total_comments: int
    total_embeddings: int
    videos_last_24h: int
    videos_last_7d: int

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜
def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    return psycopg2.connect(**DB_CONFIG)

# =============================================================================
# ğŸ” SEARCH ALGORITHMS SECTION
# =============================================================================
# ê° ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì€ ë…ë¦½ì ì¸ í•¨ìˆ˜ë¡œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
# ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€ ì‹œ ì´ ì„¹ì…˜ì— í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.

# =============================================================================
# ğŸ“Š BASIC SEARCH ALGORITHMS
# =============================================================================

def basic_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """ê¸°ë³¸ ILIKE ê²€ìƒ‰"""
    search_query = """
        SELECT 
            v.video_yid as id,
            v.title,
            v.description,
            v.published_at,
            c.title as channel_name,
            (v.statistics->>'view_count')::int as view_count,
            (v.statistics->>'like_count')::int as like_count,
            (v.statistics->>'comment_count')::int as comment_count,
            v.tags,
            v.thumbnails,
            v.privacy_status,
            v.license,
            v.embeddable,
            v.made_for_kids,
            v.recording_location,
            v.recording_date,
            v.localizations,
            v.topic_categories,
            v.relevant_topic_ids
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
        ORDER BY v.published_at DESC
        LIMIT %s OFFSET %s
    """
    
    cur.execute(search_query, (search_term, search_term, search_term, limit, offset))
    videos = cur.fetchall()
    
    # ì´ ê°œìˆ˜ ì¡°íšŒ
    count_query = """
        SELECT COUNT(*)
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
    """
    cur.execute(count_query, (search_term, search_term, search_term))
    total_count = cur.fetchone()['count']
    
    return videos, total_count

# =============================================================================
# ğŸ§® TF-IDF SEARCH ALGORITHMS
# =============================================================================

def tfidf_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """TF-IDF ê¸°ë°˜ ê²€ìƒ‰"""
    # ëª¨ë“  ë¹„ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    all_videos_query = """
        SELECT 
            v.video_yid as id,
            v.title,
            v.description,
            v.published_at,
            c.title as channel_name,
            (v.statistics->>'view_count')::int as view_count,
            (v.statistics->>'like_count')::int as like_count,
            (v.statistics->>'comment_count')::int as comment_count,
            v.tags,
            v.thumbnails,
            v.privacy_status,
            v.license,
            v.embeddable,
            v.made_for_kids,
            v.recording_location,
            v.recording_date,
            v.localizations,
            v.topic_categories,
            v.relevant_topic_ids
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
    """
    
    cur.execute(all_videos_query)
    all_videos = cur.fetchall()
    
    if not all_videos:
        return [], 0
    
    # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    documents = []
    video_ids = []
    
    for video in all_videos:
        # ì œëª©, ì„¤ëª…, íƒœê·¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•©
        doc_text = f"{video['title']} {video['description'] or ''} {' '.join(video['tags'] or [])}"
        documents.append(doc_text)
        video_ids.append(video['id'])
    
    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words=None,  # í•œêµ­ì–´ëŠ” stop words ì œê±°í•˜ì§€ ì•ŠìŒ
        ngram_range=(1, 2)  # 1-gramê³¼ 2-gram ì‚¬ìš©
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(documents)
        query_vector = vectorizer.transform([search_term])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similarity_scores = list(enumerate(similarities))
        similarity_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ê²°ê³¼ í•„í„°ë§ (ìœ ì‚¬ë„ê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ)
        filtered_results = [(idx, score) for idx, score in similarity_scores if score > 0]
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
        start_idx = offset
        end_idx = offset + limit
        paginated_results = filtered_results[start_idx:end_idx]
        
        # ê²°ê³¼ ë¹„ë””ì˜¤ ë°ì´í„° ë°˜í™˜
        result_videos = []
        for idx, score in paginated_results:
            video = all_videos[idx]
            result_videos.append(video)
        
        return result_videos, len(filtered_results)
        
    except Exception as e:
        logger.error(f"TF-IDF ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ fallback
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# âš–ï¸ WEIGHTED SEARCH ALGORITHMS
# =============================================================================

def weighted_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """í•„ë“œë³„ ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ê²€ìƒ‰"""
    # í•„ë“œë³„ ê°€ì¤‘ì¹˜
    title_weight = 3.0
    tag_weight = 2.0
    description_weight = 1.0
    
    search_query = """
        SELECT 
            v.video_yid as id,
            v.title,
            v.description,
            v.published_at,
            c.title as channel_name,
            (v.statistics->>'view_count')::int as view_count,
            (v.statistics->>'like_count')::int as like_count,
            (v.statistics->>'comment_count')::int as comment_count,
            v.tags,
            v.thumbnails,
            v.privacy_status,
            v.license,
            v.embeddable,
            v.made_for_kids,
            v.recording_location,
            v.recording_date,
            v.localizations,
            v.topic_categories,
            v.relevant_topic_ids,
            -- ê°€ì¤‘ì¹˜ ì ìˆ˜ ê³„ì‚°
            (
                CASE WHEN v.title ILIKE %s THEN %s ELSE 0 END +
                CASE WHEN v.description ILIKE %s THEN %s ELSE 0 END +
                CASE WHEN EXISTS (
                    SELECT 1 FROM unnest(v.tags) as tag 
                    WHERE tag ILIKE %s
                ) THEN %s ELSE 0 END
            ) as relevance_score
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
        ORDER BY relevance_score DESC, v.published_at DESC
        LIMIT %s OFFSET %s
    """
    
    cur.execute(search_query, (
        search_term, title_weight,
        search_term, description_weight,
        search_term, tag_weight,
        search_term, search_term, search_term,
        limit, offset
    ))
    videos = cur.fetchall()
    
    # ì´ ê°œìˆ˜ ì¡°íšŒ
    count_query = """
        SELECT COUNT(*)
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
    """
    cur.execute(count_query, (search_term, search_term, search_term))
    total_count = cur.fetchone()['count']
    
    return videos, total_count

# =============================================================================
# ğŸ” OPENSEARCH BM25 SEARCH ALGORITHMS
# =============================================================================

def opensearch_bm25_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """OpenSearch BM25 ì „ë¬¸ ê²€ìƒ‰"""
    try:
        # OpenSearchì—ì„œ BM25 ê²€ìƒ‰ ì‹¤í–‰
        search_body = {
            "query": {
                "multi_match": {
                    "query": search_term,
                    "fields": [
                        "title^3.0",      # ì œëª©ì— ë†’ì€ ê°€ì¤‘ì¹˜
                        "description^1.0", # ì„¤ëª…ì— ê¸°ë³¸ ê°€ì¤‘ì¹˜
                        "tags^2.0"        # íƒœê·¸ì— ì¤‘ê°„ ê°€ì¤‘ì¹˜
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"   # ì˜¤íƒ€ í—ˆìš©
                }
            },
            "sort": [
                {"_score": {"order": "desc"}},  # ê´€ë ¨ë„ ìˆœ
                {"published_at": {"order": "desc"}}  # ìµœì‹ ìˆœ
            ],
            "from": offset,
            "size": limit
        }
        
        # OpenSearch ê²€ìƒ‰ ì‹¤í–‰
        response = OS_CLIENT.search(
            index="videos",
            body=search_body
        )
        
        # ê²°ê³¼ì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ
        video_ids = [hit["_source"]["video_id"] for hit in response["hits"]["hits"]]
        
        if not video_ids:
            return [], 0
        
        # PostgreSQLì—ì„œ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        placeholders = ",".join(["%s"] * len(video_ids))
        detail_query = f"""
            SELECT 
                v.video_yid as id,
                v.title,
                v.description,
                v.published_at,
                c.title as channel_name,
                (v.statistics->>'view_count')::int as view_count,
                (v.statistics->>'like_count')::int as like_count,
                (v.statistics->>'comment_count')::int as comment_count,
                v.tags,
                v.thumbnails,
                v.privacy_status,
                v.license,
                v.embeddable,
                v.made_for_kids,
                v.recording_location,
                v.recording_date,
                v.localizations,
                v.topic_categories,
                v.relevant_topic_ids
            FROM yt2.videos v
            JOIN yt2.channels c ON v.channel_id = c.id
            WHERE v.video_yid IN ({placeholders})
            ORDER BY 
                CASE v.video_yid
                    {''.join([f"WHEN %s THEN {i}" for i in range(len(video_ids))])}
                END
        """
        
        cur.execute(detail_query, video_ids + video_ids)
        videos = cur.fetchall()
        
        # ì´ ê°œìˆ˜ ì¡°íšŒ (OpenSearchì—ì„œ)
        total_count = response["hits"]["total"]["value"]
        
        return videos, total_count
        
    except Exception as e:
        logger.error(f"OpenSearch BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ fallback
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# ğŸ”— HYBRID SEARCH ALGORITHMS
# =============================================================================

def hybrid_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (TF-IDF + BM25)"""
    try:
        # TF-IDF ê²€ìƒ‰ ì‹¤í–‰ (ë³„ë„ ì»¤ì„œ ì‚¬ìš©)
        with get_db_connection() as tfidf_conn:
            with tfidf_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as tfidf_cur:
                tfidf_videos, tfidf_count = tfidf_search(tfidf_cur, search_term, limit * 2, offset)
        
        # OpenSearch BM25 ê²€ìƒ‰ ì‹¤í–‰ (ë³„ë„ ì»¤ì„œ ì‚¬ìš©)
        with get_db_connection() as bm25_conn:
            with bm25_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as bm25_cur:
                bm25_videos, bm25_count = opensearch_bm25_search(bm25_cur, search_term, limit * 2, offset)
        
        # ê²°ê³¼ í•©ì¹˜ê¸° ë° ì¤‘ë³µ ì œê±°
        video_scores = {}
        
        # TF-IDF ê²°ê³¼ì— ì ìˆ˜ ë¶€ì—¬ (0.4 ê°€ì¤‘ì¹˜)
        for i, video in enumerate(tfidf_videos):
            video_id = video['id']
            score = 0.4 * (1.0 - i / len(tfidf_videos))  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
            video_scores[video_id] = video_scores.get(video_id, 0) + score
        
        # BM25 ê²°ê³¼ì— ì ìˆ˜ ë¶€ì—¬ (0.6 ê°€ì¤‘ì¹˜)
        for i, video in enumerate(bm25_videos):
            video_id = video['id']
            score = 0.6 * (1.0 - i / len(bm25_videos))  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
            video_scores[video_id] = video_scores.get(video_id, 0) + score
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_videos = sorted(video_scores.items(), key=lambda x: x[1], reverse=True)
        
        # ìµœì¢… ê²°ê³¼ ìƒì„±
        final_videos = []
        video_dict = {v['id']: v for v in tfidf_videos + bm25_videos}
        
        for video_id, score in sorted_videos[:limit]:
            if video_id in video_dict:
                final_videos.append(video_dict[video_id])
        
        return final_videos, len(video_scores)
        
    except Exception as e:
        logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ fallback
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# ğŸ§  SEMANTIC SEARCH ALGORITHMS
# =============================================================================

def semantic_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (ì„ë² ë”© ìœ ì‚¬ë„)"""
    try:
        # ì„ë² ë”©ì´ ìˆëŠ” ë¹„ë””ì˜¤ë§Œ ê²€ìƒ‰
        embedding_query = """
            SELECT 
                v.video_yid as id,
                v.title,
                v.description,
                v.published_at,
                c.title as channel_name,
                (v.statistics->>'view_count')::int as view_count,
                (v.statistics->>'like_count')::int as like_count,
                (v.statistics->>'comment_count')::int as comment_count,
                v.tags,
                v.thumbnails,
                v.privacy_status,
                v.license,
                v.embeddable,
                v.made_for_kids,
                v.recording_location,
                v.recording_date,
                v.localizations,
                v.topic_categories,
                v.relevant_topic_ids,
                e.embedding_vector
            FROM yt2.videos v
            JOIN yt2.channels c ON v.channel_id = c.id
            JOIN yt2.embeddings e ON v.id = e.video_id
            WHERE e.embedding_type = 'title'
        """
        
        cur.execute(embedding_query)
        videos_with_embeddings = cur.fetchall()
        
        if not videos_with_embeddings:
            logger.warning("ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ fallback")
            return basic_search(cur, search_term, limit, offset)
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜)
        documents = [f"{v['title']} {v['description'] or ''}" for v in videos_with_embeddings]
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        
        try:
            tfidf_matrix = vectorizer.fit_transform(documents)
            query_vector = vectorizer.transform([search_term])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarity_scores = list(enumerate(similarities))
            similarity_scores.sort(key=lambda x: x[1], reverse=True)
            
            # ê²°ê³¼ í•„í„°ë§ ë° í˜ì´ì§€ë„¤ì´ì…˜
            filtered_results = [(idx, score) for idx, score in similarity_scores if score > 0.1]
            paginated_results = filtered_results[offset:offset + limit]
            
            # ê²°ê³¼ ë¹„ë””ì˜¤ ë°˜í™˜
            result_videos = []
            for idx, score in paginated_results:
                video = videos_with_embeddings[idx]
                result_videos.append(video)
            
            return result_videos, len(filtered_results)
            
        except Exception as e:
            logger.error(f"ì˜ë¯¸ ê²€ìƒ‰ ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return basic_search(cur, search_term, limit, offset)
        
    except Exception as e:
        logger.error(f"ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# ğŸ˜Š SENTIMENT ANALYSIS SEARCH ALGORITHMS
# =============================================================================

def sentiment_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """ê°ì • ë¶„ì„ì´ í¬í•¨ëœ ê²€ìƒ‰"""
    try:
        # ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ ë¹„ë””ì˜¤ ì°¾ê¸°
        videos, total_count = basic_search(cur, search_term, limit * 2, offset)
        
        if not videos:
            return [], 0
        
        # ê° ë¹„ë””ì˜¤ì˜ ê°ì • ì ìˆ˜ ì¡°íšŒ
        video_ids = [v['id'] for v in videos]
        placeholders = ",".join(["%s"] * len(video_ids))
        
        sentiment_query = f"""
            SELECT 
                v.video_yid,
                COALESCE(AVG(c.sentiment_score), 0) as avg_sentiment,
                COUNT(c.id) as comment_count
            FROM yt2.videos v
            LEFT JOIN yt2.comments c ON v.id = c.video_id
            WHERE v.video_yid IN ({placeholders})
            GROUP BY v.video_yid
        """
        
        cur.execute(sentiment_query, video_ids)
        sentiment_data = {row['video_yid']: {
            'avg_sentiment': row['avg_sentiment'],
            'comment_count': row['comment_count']
        } for row in cur.fetchall()}
        
        # ê°ì • ì ìˆ˜ë¥¼ ê³ ë ¤í•œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
        scored_videos = []
        for video in videos:
            video_id = video['id']
            sentiment_info = sentiment_data.get(video_id, {'avg_sentiment': 0, 'comment_count': 0})
            
            # ê¸°ë³¸ ê´€ë ¨ë„ ì ìˆ˜ (ìˆœìœ„ ê¸°ë°˜)
            base_score = 1.0 - videos.index(video) / len(videos)
            
            # ê°ì • ì ìˆ˜ ë³´ë„ˆìŠ¤ (ê¸ì •ì  ëŒ“ê¸€ì´ ë§ì€ ì˜ìƒì— ê°€ì )
            sentiment_bonus = max(0, sentiment_info['avg_sentiment']) * 0.3
            
            # ëŒ“ê¸€ ìˆ˜ ë³´ë„ˆìŠ¤ (ëŒ“ê¸€ì´ ë§ì€ ì˜ìƒì— ê°€ì )
            comment_bonus = min(0.2, sentiment_info['comment_count'] / 100) * 0.2
            
            final_score = base_score + sentiment_bonus + comment_bonus
            
            scored_videos.append((video, final_score))
        
        # ìµœì¢… ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_videos.sort(key=lambda x: x[1], reverse=True)
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
        final_videos = [video for video, score in scored_videos[:limit]]
        
        return final_videos, len(scored_videos)
        
    except Exception as e:
        logger.error(f"ê°ì • ë¶„ì„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# ğŸ¯ SEARCH ALGORITHM ROUTER
# =============================================================================

def execute_search_algorithm(algorithm: str, cur, search_term: str, limit: int, offset: int) -> tuple:
    """ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ë¼ìš°í„°"""
    algorithm_map = {
        "basic": basic_search,
        "tfidf": tfidf_search,
        "weighted": weighted_search,
        "bm25": opensearch_bm25_search,
        "hybrid": hybrid_search,
        "semantic": semantic_search,
        "sentiment": sentiment_search
    }
    
    search_func = algorithm_map.get(algorithm, basic_search)
    logger.info(f"ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰: {algorithm}")
    
    return search_func(cur, search_term, limit, offset)

# =============================================================================
# ğŸ¤– AI STATISTICS & RECOMMENDATION MODELS
# =============================================================================

class VideoStats(BaseModel):
    video_id: str
    title: str
    channel_name: str
    view_count: int
    like_count: int
    comment_count: int
    published_at: str
    engagement_rate: float
    popularity_score: float

class ChannelStats(BaseModel):
    channel_id: str
    channel_name: str
    video_count: int
    total_views: int
    avg_views: float
    total_likes: int
    avg_likes: float
    engagement_rate: float

class TrendData(BaseModel):
    period: str
    video_count: int
    total_views: int
    avg_views: float
    top_keywords: List[str]

class RecommendationResponse(BaseModel):
    video_id: str
    title: str
    channel_name: str
    thumbnail_url: str
    view_count: int
    like_count: int
    published_at: str
    similarity_score: float
    recommendation_reason: str

# =============================================================================
# ğŸ“Š STATISTICS FUNCTIONS
# =============================================================================

def get_popular_videos(cur, limit: int = 10) -> List[VideoStats]:
    """ì¸ê¸° ë¹„ë””ì˜¤ í†µê³„ ì¡°íšŒ"""
    query = """
    SELECT 
        v.video_yid as video_id,
        v.title,
        c.title as channel_name,
        (v.statistics->>'view_count')::int as view_count,
        (v.statistics->>'like_count')::int as like_count,
        (v.statistics->>'comment_count')::int as comment_count,
        v.published_at,
        CASE 
            WHEN (v.statistics->>'view_count')::int > 0 
            THEN ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int * 100
            ELSE 0 
        END as engagement_rate,
        CASE 
            WHEN (v.statistics->>'view_count')::int > 0 
            THEN LOG((v.statistics->>'view_count')::int + 1) * 
                 (1 + ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int)
            ELSE 0 
        END as popularity_score
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.statistics->>'view_count' IS NOT NULL
    ORDER BY popularity_score DESC
    LIMIT %s
    """
    
    cur.execute(query, (limit,))
    results = cur.fetchall()
    
    return [
        VideoStats(
            video_id=row[0],
            title=row[1],
            channel_name=row[2],
            view_count=row[3] or 0,
            like_count=row[4] or 0,
            comment_count=row[5] or 0,
            published_at=row[6].isoformat() if row[6] else "",
            engagement_rate=round(row[7], 2),
            popularity_score=round(row[8], 2)
        )
        for row in results
    ]

def get_channel_stats(cur) -> List[ChannelStats]:
    """ì±„ë„ë³„ í†µê³„ ì¡°íšŒ"""
    query = """
    SELECT 
        c.id as channel_id,
        c.title as channel_name,
        COUNT(v.id) as video_count,
        SUM((v.statistics->>'view_count')::int) as total_views,
        AVG((v.statistics->>'view_count')::int) as avg_views,
        SUM((v.statistics->>'like_count')::int) as total_likes,
        AVG((v.statistics->>'like_count')::int) as avg_likes,
        AVG(
            CASE 
                WHEN (v.statistics->>'view_count')::int > 0 
                THEN ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int * 100
                ELSE 0 
            END
        ) as engagement_rate
    FROM yt2.channels c
    LEFT JOIN yt2.videos v ON c.id = v.channel_id
    GROUP BY c.id, c.title
    ORDER BY total_views DESC
    """
    
    cur.execute(query)
    results = cur.fetchall()
    
    return [
        ChannelStats(
            channel_id=str(row[0]),
            channel_name=row[1],
            video_count=row[2],
            total_views=row[3] or 0,
            avg_views=round(row[4] or 0, 2),
            total_likes=row[5] or 0,
            avg_likes=round(row[6] or 0, 2),
            engagement_rate=round(row[7] or 0, 2)
        )
        for row in results
    ]

def get_trend_data(cur, period: str = "month") -> List[TrendData]:
    """íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ"""
    if period == "month":
        date_format = "YYYY-MM"
        group_by = "DATE_TRUNC('month', published_at)"
    elif period == "week":
        date_format = "YYYY-\"W\"WW"
        group_by = "DATE_TRUNC('week', published_at)"
    else:  # day
        date_format = "YYYY-MM-DD"
        group_by = "DATE_TRUNC('day', published_at)"
    
    query = f"""
    SELECT 
        TO_CHAR({group_by}, '{date_format}') as period,
        COUNT(*) as video_count,
        SUM((statistics->>'view_count')::int) as total_views,
        AVG((statistics->>'view_count')::int) as avg_views
    FROM yt2.videos
    WHERE published_at IS NOT NULL
    GROUP BY {group_by}
    ORDER BY {group_by} DESC
    LIMIT 12
    """
    
    cur.execute(query)
    results = cur.fetchall()
    
    return [
        TrendData(
            period=row[0],
            video_count=row[1],
            total_views=row[2] or 0,
            avg_views=round(row[3] or 0, 2),
            top_keywords=[]  # TODO: í‚¤ì›Œë“œ ë¶„ì„ ì¶”ê°€
        )
        for row in results
    ]

# =============================================================================
# ğŸ¯ RECOMMENDATION FUNCTIONS
# =============================================================================

def get_content_based_recommendations(cur, video_id: str, limit: int = 5) -> List[RecommendationResponse]:
    """ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ (ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ë°©ì‹)"""
    # 1. ê¸°ì¤€ ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
    base_query = """
    SELECT title, description, tags
    FROM yt2.videos
    WHERE video_yid = %s
    """
    cur.execute(base_query, (video_id,))
    base_video = cur.fetchone()
    
    if not base_video:
        return []
    
    # 2. ëª¨ë“  ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
    all_query = """
    SELECT v.video_yid, v.title, v.description, v.tags, c.title as channel_name,
           v.statistics->>'view_count' as view_count,
           v.statistics->>'like_count' as like_count,
           v.published_at,
           v.thumbnails->'default'->>'url' as thumbnail_url
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.video_yid != %s
    """
    cur.execute(all_query, (video_id,))
    all_videos = cur.fetchall()
    
    if not all_videos:
        return []
    
    # 3. TF-IDF ë²¡í„°í™”
    base_text = f"{base_video[0]} {base_video[1]} {' '.join(base_video[2] or [])}"
    all_texts = [f"{row[1]} {row[2]} {' '.join(row[3] or [])}" for row in all_videos]
    
    vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform(all_texts)
    base_vector = vectorizer.transform([base_text])
    
    # 4. ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity(base_vector, tfidf_matrix).flatten()
    
    # 5. ìƒìœ„ ê²°ê³¼ ì„ íƒ
    top_indices = similarities.argsort()[-limit:][::-1]
    
    recommendations = []
    for idx in top_indices:
        if similarities[idx] > 0.1:  # ì„ê³„ê°’ ì„¤ì •
            video = all_videos[idx]
            recommendations.append(RecommendationResponse(
                video_id=video[0],
                title=video[1],
                channel_name=video[4],
                thumbnail_url=video[8] or "",
                view_count=int(video[5] or 0),
                like_count=int(video[6] or 0),
                published_at=video[7].isoformat() if video[7] else "",
                similarity_score=round(similarities[idx], 3),
                recommendation_reason="ì œëª©ê³¼ ì„¤ëª…ì´ ìœ ì‚¬í•©ë‹ˆë‹¤"
            ))
    
    return recommendations

def get_youtube_video_info(video_id: str) -> dict:
    """YouTube APIë¥¼ í†µí•´ ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ"""
    try:
        from googleapiclient.discovery import build
        
        # YouTube API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        youtube = build('youtube', 'v3', developerKey=os.getenv('YOUTUBE_API_KEY'))
        
        # ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
        request = youtube.videos().list(
            part='snippet,statistics',
            id=video_id
        )
        response = request.execute()
        
        if not response['items']:
            return None
            
        video = response['items'][0]
        snippet = video['snippet']
        statistics = video['statistics']
        
        return {
            'video_id': video_id,
            'title': snippet['title'],
            'description': snippet['description'],
            'channel_title': snippet['channelTitle'],
            'published_at': snippet['publishedAt'],
            'thumbnails': snippet['thumbnails'],
            'view_count': int(statistics.get('viewCount', 0)),
            'like_count': int(statistics.get('likeCount', 0)),
            'comment_count': int(statistics.get('commentCount', 0)),
            'tags': snippet.get('tags', [])
        }
        
    except Exception as e:
        logger.error(f"YouTube API ì˜¤ë¥˜: {e}")
        if "quotaExceeded" in str(e) or "403" in str(e):
            raise HTTPException(status_code=429, detail="YouTube API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        else:
            raise HTTPException(status_code=500, detail=f"YouTube API ì˜¤ë¥˜: {str(e)}")

def get_content_based_recommendations_with_youtube_api(cur, video_id: str, limit: int = 5) -> List[RecommendationResponse]:
    """YouTube APIë¥¼ í™œìš©í•œ ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ"""
    try:
        # 1. YouTube APIë¡œ ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
        youtube_video = get_youtube_video_info(video_id)
        if not youtube_video:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ YouTube ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
        all_query = """
        SELECT v.video_yid, v.title, v.description, v.tags, c.title as channel_name,
               v.statistics->>'view_count' as view_count,
               v.statistics->>'like_count' as like_count,
               v.published_at,
               v.thumbnails->'default'->>'url' as thumbnail_url
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        """
        cur.execute(all_query)
        all_videos = cur.fetchall()
        
        if not all_videos:
            return []
        
        # 3. YouTube ë¹„ë””ì˜¤ì™€ ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë””ì˜¤ë“¤ì„ TF-IDFë¡œ ë¹„êµ
        youtube_text = f"{youtube_video['title']} {youtube_video['description']} {' '.join(youtube_video['tags'])}"
        db_texts = [f"{row[1]} {row[2]} {' '.join(row[3] or [])}" for row in all_videos]
        
        # 4. TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        all_texts = [youtube_text] + db_texts
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        # 5. ìœ ì‚¬ë„ ê³„ì‚°
        youtube_vector = tfidf_matrix[0:1]
        db_vectors = tfidf_matrix[1:]
        similarities = cosine_similarity(youtube_vector, db_vectors).flatten()
        
        # 6. ìƒìœ„ ê²°ê³¼ ì„ íƒ
        top_indices = similarities.argsort()[-limit:][::-1]
        
        recommendations = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ì„ê³„ê°’ ì„¤ì •
                video = all_videos[idx]
                recommendations.append(RecommendationResponse(
                    video_id=video[0],
                    title=video[1],
                    channel_name=video[4],
                    thumbnail_url=video[8] or "",
                    view_count=int(video[5] or 0),
                    like_count=int(video[6] or 0),
                    published_at=video[7].isoformat() if video[7] else "",
                    similarity_score=round(similarities[idx], 3),
                    recommendation_reason=f"'{youtube_video['title']}'ì™€ ìœ ì‚¬í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤"
                ))
        
        return recommendations
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def get_popularity_based_recommendations(cur, limit: int = 5) -> List[RecommendationResponse]:
    """ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œ"""
    query = """
    SELECT 
        v.video_yid,
        v.title,
        c.title as channel_name,
        v.statistics->>'view_count' as view_count,
        v.statistics->>'like_count' as like_count,
        v.published_at,
        v.thumbnails->'default'->>'url' as thumbnail_url,
        CASE 
            WHEN (v.statistics->>'view_count')::int > 0 
            THEN LOG((v.statistics->>'view_count')::int + 1) * 
                 (1 + ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int)
            ELSE 0 
        END as popularity_score
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.statistics->>'view_count' IS NOT NULL
    ORDER BY popularity_score DESC
    LIMIT %s
    """
    
    cur.execute(query, (limit,))
    results = cur.fetchall()
    
    return [
        RecommendationResponse(
            video_id=row[0],
            title=row[1],
            channel_name=row[2],
            thumbnail_url=row[6] or "",
            view_count=int(row[3] or 0),
            like_count=int(row[4] or 0),
            published_at=row[5].isoformat() if row[5] else "",
            similarity_score=round(row[7], 3),
            recommendation_reason="ë†’ì€ ì¸ê¸°ë„ë¥¼ ë³´ì…ë‹ˆë‹¤"
        )
        for row in results
    ]

def get_trending_recommendations(cur, limit: int = 5) -> List[RecommendationResponse]:
    """ìµœì‹  íŠ¸ë Œë“œ ì¶”ì²œ"""
    query = """
    SELECT 
        v.video_yid,
        v.title,
        c.title as channel_name,
        v.statistics->>'view_count' as view_count,
        v.statistics->>'like_count' as like_count,
        v.published_at,
        v.thumbnails->'default'->>'url' as thumbnail_url,
        EXTRACT(EPOCH FROM (NOW() - v.published_at)) / 86400 as days_ago
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.published_at IS NOT NULL
    ORDER BY v.published_at DESC
    LIMIT %s
    """
    
    cur.execute(query, (limit,))
    results = cur.fetchall()
    
    return [
        RecommendationResponse(
            video_id=row[0],
            title=row[1],
            channel_name=row[2],
            thumbnail_url=row[6] or "",
            view_count=int(row[3] or 0),
            like_count=int(row[4] or 0),
            published_at=row[5].isoformat() if row[5] else "",
            similarity_score=round(1.0 / (1.0 + float(row[7] or 0)), 3),  # decimal.Decimalì„ floatë¡œ ë³€í™˜
            recommendation_reason="ìµœì‹  ì½˜í…ì¸ ì…ë‹ˆë‹¤"
        )
        for row in results
    ]

# =============================================================================
# ğŸŒ API ENDPOINTS
# =============================================================================
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "YT2 API ì„œë²„",
        "description": "ìˆ˜ì›ì‹œ í–‰ê¶ë™ YouTube ë°ì´í„° ê²€ìƒ‰ API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
        
        # OpenSearch ì—°ê²° í™•ì¸
        OS_CLIENT.ping()
        
        # Redis ì—°ê²° í™•ì¸
        REDIS_CLIENT.ping()
        
        return {
            "status": "healthy",
            "database": "connected",
            "opensearch": "connected",
            "redis": "connected",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë¹„ìŠ¤ ìƒíƒœ ë¶ˆëŸ‰: {str(e)}")

@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM yt2.channels) as total_channels,
                        (SELECT COUNT(*) FROM yt2.videos) as total_videos,
                        (SELECT COUNT(*) FROM yt2.comments) as total_comments,
                        (SELECT COUNT(*) FROM yt2.embeddings) as total_embeddings,
                        (SELECT COUNT(*) FROM yt2.videos WHERE created_at >= NOW() - INTERVAL '24 hours') as videos_last_24h,
                        (SELECT COUNT(*) FROM yt2.videos WHERE created_at >= NOW() - INTERVAL '7 days') as videos_last_7d
                """)
                
                stats = cur.fetchone()
                
                return StatsResponse(
                    total_channels=stats[0],
                    total_videos=stats[1],
                    total_comments=stats[2],
                    total_embeddings=stats[3],
                    videos_last_24h=stats[4],
                    videos_last_7d=stats[5]
                )
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/search", response_model=SearchResponse)
async def search_videos(
    q: str = Query(..., description="ê²€ìƒ‰ì–´"),
    limit: int = Query(10, ge=1, le=100, description="ê²°ê³¼ ìˆ˜ ì œí•œ"),
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    algorithm: str = Query("basic", description="ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜"),
    offset: int = Query(0, ge=0, description="ê²°ê³¼ ì˜¤í”„ì…‹")
):
    """ì˜ìƒ ê²€ìƒ‰"""
    start_time = datetime.now()
    
    # í˜ì´ì§€ ê¸°ë°˜ ì˜¤í”„ì…‹ ê³„ì‚°
    actual_offset = (page - 1) * limit if page > 0 else offset
    
    try:
        # ìºì‹œ í™•ì¸
        cache_key = f"search:{q}:{limit}:{page}:{algorithm}"
        cached_result = REDIS_CLIENT.get(cache_key)
        if cached_result:
            logger.info(f"ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜: {q} (ì•Œê³ ë¦¬ì¦˜: {algorithm})")
            return json.loads(cached_result)
        
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                search_term = f"%{q}%"
                
                # ğŸ¯ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
                videos, total_count = execute_search_algorithm(algorithm, cur, search_term, limit, actual_offset)
                
                # ê²°ê³¼ ë³€í™˜
                video_responses = []
                for video in videos:
                    video_responses.append(VideoResponse(
                        id=video['id'],
                        title=video['title'],
                        description=video['description'],
                        published_at=video['published_at'].isoformat() if video['published_at'] else None,
                        channel_name=video['channel_name'],
                        view_count=video['view_count'] or 0,
                        like_count=video['like_count'] or 0,
                        comment_count=video['comment_count'] or 0,
                        tags=video['tags'] or [],
                        thumbnails=video['thumbnails'] or {},
                        # ì¶”ê°€ëœ í•„ë“œë“¤
                        privacy_status=video.get('privacy_status'),
                        license=video.get('license'),
                        embeddable=video.get('embeddable'),
                        made_for_kids=video.get('made_for_kids'),
                        recording_location=video.get('recording_location'),
                        recording_date=video['recording_date'].isoformat() if video.get('recording_date') else None,
                        localizations=video.get('localizations'),
                        topic_categories=video.get('topic_categories') or [],
                        relevant_topic_ids=video.get('relevant_topic_ids') or []
                    ))
                
                search_time = (datetime.now() - start_time).total_seconds()
                total_pages = (total_count + limit - 1) // limit  # ì˜¬ë¦¼ ê³„ì‚°
                
                result = SearchResponse(
                    videos=video_responses,
                    total_count=total_count,
                    total_pages=total_pages,
                    query=q,
                    search_time=search_time
                )
                
                # ìºì‹œ ì €ì¥ (5ë¶„)
                REDIS_CLIENT.setex(cache_key, 300, json.dumps(result.dict(), default=str))
                
                # ê²€ìƒ‰ ë¡œê·¸ ì €ì¥
                log_search(q, len(video_responses), search_time)
                
                return result
                
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/videos/{video_id}")
async def get_video_detail(video_id: str):
    """ì˜ìƒ ìƒì„¸ ì •ë³´"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        v.video_yid as id,
                        v.title,
                        v.description,
                        v.published_at,
                        v.duration,
                        v.statistics,
                        v.tags,
                        v.thumbnails,
                        c.title as channel_name,
                        c.description as channel_description,
                        c.statistics as channel_statistics
                    FROM yt2.videos v
                    JOIN yt2.channels c ON v.channel_id = c.id
                    WHERE v.video_yid = %s
                """, (video_id,))
                
                video = cur.fetchone()
                if not video:
                    raise HTTPException(status_code=404, detail="ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                return {
                    "id": video['id'],
                    "title": video['title'],
                    "description": video['description'],
                    "published_at": video['published_at'].isoformat() if video['published_at'] else None,
                    "duration": video['duration'],
                    "statistics": video['statistics'],
                    "tags": video['tags'],
                    "thumbnails": video['thumbnails'],
                    "channel": {
                        "name": video['channel_name'],
                        "description": video['channel_description'],
                        "statistics": video['channel_statistics']
                    }
                }
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì˜ìƒ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì˜ìƒ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/channels")
async def get_channels(
    limit: int = Query(10, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """ì±„ë„ ëª©ë¡"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        c.channel_yid as id,
                        c.title,
                        c.description,
                        c.statistics,
                        c.thumbnails,
                        COUNT(v.id) as video_count
                    FROM yt2.channels c
                    LEFT JOIN yt2.videos v ON c.id = v.channel_id
                    GROUP BY c.id, c.channel_yid, c.title, c.description, c.statistics, c.thumbnails
                    ORDER BY video_count DESC
                    LIMIT %s OFFSET %s
                """, (limit, offset))
                
                channels = cur.fetchall()
                
                return [
                    {
                        "id": channel['id'],
                        "title": channel['title'],
                        "description": channel['description'],
                        "statistics": channel['statistics'],
                        "thumbnails": channel['thumbnails'],
                        "video_count": channel['video_count']
                    }
                    for channel in channels
                ]
                
    except Exception as e:
        logger.error(f"ì±„ë„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì±„ë„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/playlists")
async def get_playlists(
    channel_id: Optional[str] = Query(None, description="ì±„ë„ ID"),
    limit: int = Query(10, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """ì¬ìƒëª©ë¡ ëª©ë¡"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                if channel_id:
                    cur.execute("""
                        SELECT 
                            p.playlist_yid as id,
                            p.title,
                            p.description,
                            p.thumbnails,
                            p.item_count,
                            p.privacy_status,
                            p.localizations,
                            c.title as channel_name
                        FROM yt2.playlists p
                        JOIN yt2.channels c ON p.channel_id = c.id
                        WHERE c.channel_yid = %s
                        ORDER BY p.created_at DESC
                        LIMIT %s OFFSET %s
                    """, (channel_id, limit, offset))
                else:
                    cur.execute("""
                        SELECT 
                            p.playlist_yid as id,
                            p.title,
                            p.description,
                            p.thumbnails,
                            p.item_count,
                            p.privacy_status,
                            p.localizations,
                            c.title as channel_name
                        FROM yt2.playlists p
                        JOIN yt2.channels c ON p.channel_id = c.id
                        ORDER BY p.created_at DESC
                        LIMIT %s OFFSET %s
                    """, (limit, offset))
                
                playlists = cur.fetchall()
                
                return [
                    {
                        "id": playlist['id'],
                        "title": playlist['title'],
                        "description": playlist['description'],
                        "thumbnails": playlist['thumbnails'],
                        "item_count": playlist['item_count'],
                        "privacy_status": playlist['privacy_status'],
                        "localizations": playlist['localizations'],
                        "channel_name": playlist['channel_name']
                    }
                    for playlist in playlists
                ]
                
    except Exception as e:
        logger.error(f"ì¬ìƒëª©ë¡ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬ìƒëª©ë¡ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/playlists/{playlist_id}/items")
async def get_playlist_items(
    playlist_id: str,
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """ì¬ìƒëª©ë¡ ì•„ì´í…œ ëª©ë¡"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        pi.playlist_item_yid as id,
                        pi.position,
                        pi.title,
                        pi.description,
                        pi.thumbnails,
                        pi.published_at,
                        v.video_yid as video_id,
                        v.title as video_title,
                        v.duration,
                        v.statistics
                    FROM yt2.playlist_items pi
                    LEFT JOIN yt2.videos v ON pi.video_id = v.id
                    WHERE pi.playlist_id = (
                        SELECT id FROM yt2.playlists WHERE playlist_yid = %s
                    )
                    ORDER BY pi.position
                    LIMIT %s OFFSET %s
                """, (playlist_id, limit, offset))
                
                items = cur.fetchall()
                
                return [
                    {
                        "id": item['id'],
                        "position": item['position'],
                        "title": item['title'],
                        "description": item['description'],
                        "thumbnails": item['thumbnails'],
                        "published_at": item['published_at'].isoformat() if item['published_at'] else None,
                        "video_id": item['video_id'],
                        "video_title": item['video_title'],
                        "duration": item['duration'],
                        "statistics": item['statistics']
                    }
                    for item in items
                ]
                
    except Exception as e:
        logger.error(f"ì¬ìƒëª©ë¡ ì•„ì´í…œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì¬ìƒëª©ë¡ ì•„ì´í…œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/videos/{video_id}/captions")
async def get_video_captions(video_id: str):
    """ì˜ìƒ ìë§‰ ëª©ë¡"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        caption_yid as id,
                        language,
                        name,
                        status,
                        track_kind,
                        is_auto_synced,
                        is_cc,
                        is_draft,
                        is_served,
                        is_auto_generated,
                        last_updated
                    FROM yt2.captions
                    WHERE video_id = (
                        SELECT id FROM yt2.videos WHERE video_yid = %s
                    )
                    ORDER BY language
                """, (video_id,))
                
                captions = cur.fetchall()
                
                return [
                    {
                        "id": caption['id'],
                        "language": caption['language'],
                        "name": caption['name'],
                        "status": caption['status'],
                        "track_kind": caption['track_kind'],
                        "is_auto_synced": caption['is_auto_synced'],
                        "is_cc": caption['is_cc'],
                        "is_draft": caption['is_draft'],
                        "is_served": caption['is_served'],
                        "is_auto_generated": caption['is_auto_generated'],
                        "last_updated": caption['last_updated'].isoformat() if caption['last_updated'] else None
                    }
                    for caption in captions
                ]
                
    except Exception as e:
        logger.error(f"ì˜ìƒ ìë§‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì˜ìƒ ìë§‰ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/categories")
async def get_video_categories():
    """ì˜ìƒ ì¹´í…Œê³ ë¦¬ ëª©ë¡"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        category_yid as id,
                        title,
                        assignable,
                        channel_id
                    FROM yt2.video_categories
                    ORDER BY category_yid
                """)
                
                categories = cur.fetchall()
                
                return [
                    {
                        "id": category['id'],
                        "title": category['title'],
                        "assignable": category['assignable'],
                        "channel_id": category['channel_id']
                    }
                    for category in categories
                ]
                
    except Exception as e:
        logger.error(f"ì˜ìƒ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì˜ìƒ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

def log_search(query: str, result_count: int, search_time: float):
    """ê²€ìƒ‰ ë¡œê·¸ ì €ì¥"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO yt2.search_logs (query, results_count, response_time_ms)
                    VALUES (%s, %s, %s)
                """, (query, result_count, int(search_time * 1000)))
                conn.commit()
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# =============================================================================
# ğŸ¤– AI STATISTICS & RECOMMENDATION API ENDPOINTS
# =============================================================================

@app.get("/api/stats/popular-videos", response_model=List[VideoStats])
async def get_popular_videos_api(limit: int = Query(10, ge=1, le=50, description="ê²°ê³¼ ìˆ˜ ì œí•œ")):
    """ì¸ê¸° ë¹„ë””ì˜¤ í†µê³„ ì¡°íšŒ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_popular_videos(cur, limit)
    except Exception as e:
        logger.error(f"ì¸ê¸° ë¹„ë””ì˜¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì¸ê¸° ë¹„ë””ì˜¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/stats/channels", response_model=List[ChannelStats])
async def get_channel_stats_api():
    """ì±„ë„ë³„ í†µê³„ ì¡°íšŒ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_channel_stats(cur)
    except Exception as e:
        logger.error(f"ì±„ë„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì±„ë„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/stats/trends", response_model=List[TrendData])
async def get_trend_data_api(period: str = Query("month", description="ê¸°ê°„ (day, week, month)")):
    """íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_trend_data(cur, period)
    except Exception as e:
        logger.error(f"íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/recommendations/content-based", response_model=List[RecommendationResponse])
async def get_content_based_recommendations_api(
    video_id: str = Query(..., description="ê¸°ì¤€ ë¹„ë””ì˜¤ ID"),
    limit: int = Query(5, ge=1, le=20, description="ì¶”ì²œ ìˆ˜ ì œí•œ")
):
    """ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ (ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ë°©ì‹)"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_content_based_recommendations(cur, video_id, limit)
    except Exception as e:
        logger.error(f"ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/recommendations/content-based-youtube", response_model=List[RecommendationResponse])
async def get_content_based_recommendations_youtube_api(
    video_id: str = Query(..., description="YouTube ë¹„ë””ì˜¤ ID"),
    limit: int = Query(5, ge=1, le=20, description="ì¶”ì²œ ìˆ˜ ì œí•œ")
):
    """YouTube APIë¥¼ í™œìš©í•œ ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_content_based_recommendations_with_youtube_api(cur, video_id, limit)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"YouTube API ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"YouTube API ì½˜í…ì¸  ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/recommendations/popularity", response_model=List[RecommendationResponse])
async def get_popularity_recommendations_api(
    limit: int = Query(5, ge=1, le=20, description="ì¶”ì²œ ìˆ˜ ì œí•œ")
):
    """ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_popularity_based_recommendations(cur, limit)
    except Exception as e:
        logger.error(f"ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/recommendations/trending", response_model=List[RecommendationResponse])
async def get_trending_recommendations_api(
    limit: int = Query(5, ge=1, le=20, description="ì¶”ì²œ ìˆ˜ ì œí•œ")
):
    """ìµœì‹  íŠ¸ë Œë“œ ì¶”ì²œ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_trending_recommendations(cur, limit)
    except Exception as e:
        logger.error(f"íŠ¸ë Œë“œ ì¶”ì²œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŠ¸ë Œë“œ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/stats/overview")
async def get_stats_overview():
    """í†µê³„ ê°œìš” ì¡°íšŒ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                # ì „ì²´ í†µê³„
                cur.execute("""
                    SELECT 
                        COUNT(*) as total_videos,
                        COUNT(DISTINCT channel_id) as total_channels,
                        SUM((statistics->>'view_count')::int) as total_views,
                        AVG((statistics->>'view_count')::int) as avg_views,
                        SUM((statistics->>'like_count')::int) as total_likes,
                        AVG((statistics->>'like_count')::int) as avg_likes
                    FROM yt2.videos
                    WHERE statistics->>'view_count' IS NOT NULL
                """)
                overall_stats = cur.fetchone()
                
                # ìµœê·¼ 7ì¼ í†µê³„
                cur.execute("""
                    SELECT 
                        COUNT(*) as recent_videos,
                        SUM((statistics->>'view_count')::int) as recent_views
                    FROM yt2.videos
                    WHERE published_at >= NOW() - INTERVAL '7 days'
                    AND statistics->>'view_count' IS NOT NULL
                """)
                recent_stats = cur.fetchone()
                
                return {
                    "overall": {
                        "total_videos": overall_stats[0] or 0,
                        "total_channels": overall_stats[1] or 0,
                        "total_views": overall_stats[2] or 0,
                        "avg_views": round(overall_stats[3] or 0, 2),
                        "total_likes": overall_stats[4] or 0,
                        "avg_likes": round(overall_stats[5] or 0, 2)
                    },
                    "recent_7_days": {
                        "new_videos": recent_stats[0] or 0,
                        "new_views": recent_stats[1] or 0
                    }
                }
    except Exception as e:
        logger.error(f"í†µê³„ ê°œìš” ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ê°œìš” ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
