#!/usr/bin/env python3
"""
YT2 API ÏÑúÎ≤Ñ
ÏàòÏõêÏãú ÌñâÍ∂ÅÎèô YouTube Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÉâ API
"""

import os
import sys
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
import psycopg2
import psycopg2.extras
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from opensearchpy import OpenSearch
import redis
from dotenv import load_dotenv

# Î®∏Ïã†Îü¨Îãù ÎùºÏù¥Î∏åÎü¨Î¶¨
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# from sentence_transformers import SentenceTransformer  # ÏùòÏ°¥ÏÑ± Î¨∏Ï†úÎ°ú ÏûÑÏãú ÎπÑÌôúÏÑ±Ìôî

# ÌôòÍ≤ΩÎ≥ÄÏàò Î°úÎî©
load_dotenv()

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI Ïï± ÏÉùÏÑ±
app = FastAPI(
    title="YT2 API",
    description="ÏàòÏõêÏãú ÌñâÍ∂ÅÎèô YouTube Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÉâ API",
    version="1.0.0"
)

# CORS ÏÑ§Ï†ï
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ§Ï†ï
DB_CONFIG = {
    'host': os.getenv("DB_HOST", "localhost"),
    'port': int(os.getenv("DB_PORT", "5432")),
    'dbname': os.getenv("DB_NAME", "yt2"),
    'user': os.getenv("DB_USER", "app"),
    'password': os.getenv("DB_PASSWORD", "app1234"),
}

# OpenSearch ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
OS_CLIENT = OpenSearch(
    hosts=[os.getenv("OS_HOST", "http://localhost:9200")],
    http_auth=(os.getenv("OS_USER", "admin"), os.getenv("OS_PASSWORD", "App1234!@#")),
    use_ssl=False,
    verify_certs=False
)

# Redis ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
REDIS_CLIENT = redis.Redis(
    host=os.getenv("REDIS_HOST", "localhost"),
    port=int(os.getenv("REDIS_PORT", "6379")),
    decode_responses=True
)

# Pydantic Î™®Îç∏
class VideoResponse(BaseModel):
    id: str
    title: str
    description: Optional[str]
    published_at: Optional[str]
    channel_name: str
    view_count: int
    like_count: int
    comment_count: int
    tags: List[str]
    thumbnails: Dict
    # Ï∂îÍ∞ÄÎêú ÌïÑÎìúÎì§
    privacy_status: Optional[str]
    license: Optional[str]
    embeddable: Optional[bool]
    made_for_kids: Optional[bool]
    recording_location: Optional[Dict]
    recording_date: Optional[str]
    localizations: Optional[Dict]
    topic_categories: List[str]
    relevant_topic_ids: List[str]

class SearchResponse(BaseModel):
    videos: List[VideoResponse]
    total_count: int
    total_pages: int
    query: str
    search_time: float

class StatsResponse(BaseModel):
    total_channels: int
    total_videos: int
    total_comments: int
    total_embeddings: int
    videos_last_24h: int
    videos_last_7d: int

# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ìï®Ïàò
def get_db_connection():
    """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞"""
    return psycopg2.connect(**DB_CONFIG)

# =============================================================================
# üîç SEARCH ALGORITHMS SECTION
# =============================================================================
# Í∞Å Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶òÏùÄ ÎèÖÎ¶ΩÏ†ÅÏù∏ Ìï®ÏàòÎ°ú Íµ¨ÌòÑÎêòÏñ¥ ÏûàÏäµÎãàÎã§.
# ÏÉàÎ°úÏö¥ ÏïåÍ≥†Î¶¨Ï¶ò Ï∂îÍ∞Ä Ïãú Ïù¥ ÏÑπÏÖòÏóê Ìï®ÏàòÎ•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.

# =============================================================================
# üìä BASIC SEARCH ALGORITHMS
# =============================================================================

def basic_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """Í∏∞Î≥∏ ILIKE Í≤ÄÏÉâ"""
    search_query = """
        SELECT 
            v.video_yid as id,
            v.title,
            v.description,
            v.published_at,
            c.title as channel_name,
            (v.statistics->>'view_count')::int as view_count,
            (v.statistics->>'like_count')::int as like_count,
            (v.statistics->>'comment_count')::int as comment_count,
            v.tags,
            v.thumbnails,
            v.privacy_status,
            v.license,
            v.embeddable,
            v.made_for_kids,
            v.recording_location,
            v.recording_date,
            v.localizations,
            v.topic_categories,
            v.relevant_topic_ids
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
        ORDER BY v.published_at DESC
        LIMIT %s OFFSET %s
    """
    
    cur.execute(search_query, (search_term, search_term, search_term, limit, offset))
    videos = cur.fetchall()
    
    # Ï¥ù Í∞úÏàò Ï°∞Ìöå
    count_query = """
        SELECT COUNT(*)
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
    """
    cur.execute(count_query, (search_term, search_term, search_term))
    total_count = cur.fetchone()['count']
    
    return videos, total_count

# =============================================================================
# üßÆ TF-IDF SEARCH ALGORITHMS
# =============================================================================

def tfidf_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """TF-IDF Í∏∞Î∞ò Í≤ÄÏÉâ"""
    # Î™®Îì† ÎπÑÎîîÏò§ Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
    all_videos_query = """
        SELECT 
            v.video_yid as id,
            v.title,
            v.description,
            v.published_at,
            c.title as channel_name,
            (v.statistics->>'view_count')::int as view_count,
            (v.statistics->>'like_count')::int as like_count,
            (v.statistics->>'comment_count')::int as comment_count,
            v.tags,
            v.thumbnails,
            v.privacy_status,
            v.license,
            v.embeddable,
            v.made_for_kids,
            v.recording_location,
            v.recording_date,
            v.localizations,
            v.topic_categories,
            v.relevant_topic_ids
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
    """
    
    cur.execute(all_videos_query)
    all_videos = cur.fetchall()
    
    if not all_videos:
        return [], 0
    
    # ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
    documents = []
    video_ids = []
    
    for video in all_videos:
        # Ï†úÎ™©, ÏÑ§Î™Ö, ÌÉúÍ∑∏Î•º ÌïòÎÇòÏùò Î¨∏ÏÑúÎ°ú Í≤∞Ìï©
        doc_text = f"{video['title']} {video['description'] or ''} {' '.join(video['tags'] or [])}"
        documents.append(doc_text)
        video_ids.append(video['id'])
    
    # TF-IDF Î≤°ÌÑ∞Ìôî
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words=None,  # ÌïúÍµ≠Ïñ¥Îäî stop words Ï†úÍ±∞ÌïòÏßÄ ÏïäÏùå
        ngram_range=(1, 2)  # 1-gramÍ≥º 2-gram ÏÇ¨Ïö©
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(documents)
        query_vector = vectorizer.transform([search_term])
        
        # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        
        # Ïú†ÏÇ¨ÎèÑ ÏàúÏúºÎ°ú Ï†ïÎ†¨
        similarity_scores = list(enumerate(similarities))
        similarity_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Í≤∞Í≥º ÌïÑÌÑ∞ÎßÅ (Ïú†ÏÇ¨ÎèÑÍ∞Ä 0Î≥¥Îã§ ÌÅ∞ Í≤ÉÎßå)
        filtered_results = [(idx, score) for idx, score in similarity_scores if score > 0]
        
        # ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò Ï†ÅÏö©
        start_idx = offset
        end_idx = offset + limit
        paginated_results = filtered_results[start_idx:end_idx]
        
        # Í≤∞Í≥º ÎπÑÎîîÏò§ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò
        result_videos = []
        for idx, score in paginated_results:
            video = all_videos[idx]
            result_videos.append(video)
        
        return result_videos, len(filtered_results)
        
    except Exception as e:
        logger.error(f"TF-IDF Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        # Ïã§Ìå® Ïãú Í∏∞Î≥∏ Í≤ÄÏÉâÏúºÎ°ú fallback
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# ‚öñÔ∏è WEIGHTED SEARCH ALGORITHMS
# =============================================================================

def weighted_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """ÌïÑÎìúÎ≥Ñ Í∞ÄÏ§ëÏπòÍ∞Ä Ï†ÅÏö©Îêú Í≤ÄÏÉâ"""
    # ÌïÑÎìúÎ≥Ñ Í∞ÄÏ§ëÏπò
    title_weight = 3.0
    tag_weight = 2.0
    description_weight = 1.0
    
    search_query = """
        SELECT 
            v.video_yid as id,
            v.title,
            v.description,
            v.published_at,
            c.title as channel_name,
            (v.statistics->>'view_count')::int as view_count,
            (v.statistics->>'like_count')::int as like_count,
            (v.statistics->>'comment_count')::int as comment_count,
            v.tags,
            v.thumbnails,
            v.privacy_status,
            v.license,
            v.embeddable,
            v.made_for_kids,
            v.recording_location,
            v.recording_date,
            v.localizations,
            v.topic_categories,
            v.relevant_topic_ids,
            -- Í∞ÄÏ§ëÏπò Ï†êÏàò Í≥ÑÏÇ∞
            (
                CASE WHEN v.title ILIKE %s THEN %s ELSE 0 END +
                CASE WHEN v.description ILIKE %s THEN %s ELSE 0 END +
                CASE WHEN EXISTS (
                    SELECT 1 FROM unnest(v.tags) as tag 
                    WHERE tag ILIKE %s
                ) THEN %s ELSE 0 END
            ) as relevance_score
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
        ORDER BY relevance_score DESC, v.published_at DESC
        LIMIT %s OFFSET %s
    """
    
    cur.execute(search_query, (
        search_term, title_weight,
        search_term, description_weight,
        search_term, tag_weight,
        search_term, search_term, search_term,
        limit, offset
    ))
    videos = cur.fetchall()
    
    # Ï¥ù Í∞úÏàò Ï°∞Ìöå
    count_query = """
        SELECT COUNT(*)
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        WHERE 
            v.title ILIKE %s OR 
            v.description ILIKE %s OR 
            EXISTS (
                SELECT 1 FROM unnest(v.tags) as tag 
                WHERE tag ILIKE %s
            )
    """
    cur.execute(count_query, (search_term, search_term, search_term))
    total_count = cur.fetchone()['count']
    
    return videos, total_count

# =============================================================================
# üîç OPENSEARCH BM25 SEARCH ALGORITHMS
# =============================================================================

def opensearch_bm25_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """OpenSearch BM25 Ï†ÑÎ¨∏ Í≤ÄÏÉâ"""
    try:
        # OpenSearchÏóêÏÑú BM25 Í≤ÄÏÉâ Ïã§Ìñâ
        search_body = {
            "query": {
                "multi_match": {
                    "query": search_term,
                    "fields": [
                        "title^3.0",      # Ï†úÎ™©Ïóê ÎÜíÏùÄ Í∞ÄÏ§ëÏπò
                        "description^1.0", # ÏÑ§Î™ÖÏóê Í∏∞Î≥∏ Í∞ÄÏ§ëÏπò
                        "tags^2.0"        # ÌÉúÍ∑∏Ïóê Ï§ëÍ∞Ñ Í∞ÄÏ§ëÏπò
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"   # Ïò§ÌÉÄ ÌóàÏö©
                }
            },
            "sort": [
                {"_score": {"order": "desc"}},  # Í¥ÄÎ†®ÎèÑ Ïàú
                {"published_at": {"order": "desc"}}  # ÏµúÏã†Ïàú
            ],
            "from": offset,
            "size": limit
        }
        
        # OpenSearch Í≤ÄÏÉâ Ïã§Ìñâ
        response = OS_CLIENT.search(
            index="videos",
            body=search_body
        )
        
        # Í≤∞Í≥ºÏóêÏÑú ÎπÑÎîîÏò§ ID Ï∂îÏ∂ú
        video_ids = [hit["_source"]["video_id"] for hit in response["hits"]["hits"]]
        
        if not video_ids:
            return [], 0
        
        # PostgreSQLÏóêÏÑú ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï°∞Ìöå
        placeholders = ",".join(["%s"] * len(video_ids))
        detail_query = f"""
            SELECT 
                v.video_yid as id,
                v.title,
                v.description,
                v.published_at,
                c.title as channel_name,
                (v.statistics->>'view_count')::int as view_count,
                (v.statistics->>'like_count')::int as like_count,
                (v.statistics->>'comment_count')::int as comment_count,
                v.tags,
                v.thumbnails,
                v.privacy_status,
                v.license,
                v.embeddable,
                v.made_for_kids,
                v.recording_location,
                v.recording_date,
                v.localizations,
                v.topic_categories,
                v.relevant_topic_ids
            FROM yt2.videos v
            JOIN yt2.channels c ON v.channel_id = c.id
            WHERE v.video_yid IN ({placeholders})
            ORDER BY 
                CASE v.video_yid
                    {''.join([f"WHEN %s THEN {i}" for i in range(len(video_ids))])}
                END
        """
        
        cur.execute(detail_query, video_ids + video_ids)
        videos = cur.fetchall()
        
        # Ï¥ù Í∞úÏàò Ï°∞Ìöå (OpenSearchÏóêÏÑú)
        total_count = response["hits"]["total"]["value"]
        
        return videos, total_count
        
    except Exception as e:
        logger.error(f"OpenSearch BM25 Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        # Ïã§Ìå® Ïãú Í∏∞Î≥∏ Í≤ÄÏÉâÏúºÎ°ú fallback
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# üîó HYBRID SEARCH ALGORITHMS
# =============================================================================

def hybrid_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ (TF-IDF + BM25)"""
    try:
        # TF-IDF Í≤ÄÏÉâ Ïã§Ìñâ (Î≥ÑÎèÑ Ïª§ÏÑú ÏÇ¨Ïö©)
        with get_db_connection() as tfidf_conn:
            with tfidf_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as tfidf_cur:
                tfidf_videos, tfidf_count = tfidf_search(tfidf_cur, search_term, limit * 2, offset)
        
        # OpenSearch BM25 Í≤ÄÏÉâ Ïã§Ìñâ (Î≥ÑÎèÑ Ïª§ÏÑú ÏÇ¨Ïö©)
        with get_db_connection() as bm25_conn:
            with bm25_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as bm25_cur:
                bm25_videos, bm25_count = opensearch_bm25_search(bm25_cur, search_term, limit * 2, offset)
        
        # Í≤∞Í≥º Ìï©ÏπòÍ∏∞ Î∞è Ï§ëÎ≥µ Ï†úÍ±∞
        video_scores = {}
        
        # TF-IDF Í≤∞Í≥ºÏóê Ï†êÏàò Î∂ÄÏó¨ (0.4 Í∞ÄÏ§ëÏπò)
        for i, video in enumerate(tfidf_videos):
            video_id = video['id']
            score = 0.4 * (1.0 - i / len(tfidf_videos))  # ÏàúÏúÑ Í∏∞Î∞ò Ï†êÏàò
            video_scores[video_id] = video_scores.get(video_id, 0) + score
        
        # BM25 Í≤∞Í≥ºÏóê Ï†êÏàò Î∂ÄÏó¨ (0.6 Í∞ÄÏ§ëÏπò)
        for i, video in enumerate(bm25_videos):
            video_id = video['id']
            score = 0.6 * (1.0 - i / len(bm25_videos))  # ÏàúÏúÑ Í∏∞Î∞ò Ï†êÏàò
            video_scores[video_id] = video_scores.get(video_id, 0) + score
        
        # Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨
        sorted_videos = sorted(video_scores.items(), key=lambda x: x[1], reverse=True)
        
        # ÏµúÏ¢Ö Í≤∞Í≥º ÏÉùÏÑ±
        final_videos = []
        video_dict = {v['id']: v for v in tfidf_videos + bm25_videos}
        
        for video_id, score in sorted_videos[:limit]:
            if video_id in video_dict:
                final_videos.append(video_dict[video_id])
        
        return final_videos, len(video_scores)
        
    except Exception as e:
        logger.error(f"ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        # Ïã§Ìå® Ïãú Í∏∞Î≥∏ Í≤ÄÏÉâÏúºÎ°ú fallback
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# üß† SEMANTIC SEARCH ALGORITHMS
# =============================================================================

def semantic_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """ÏùòÎØ∏ Í∏∞Î∞ò Í≤ÄÏÉâ (ÏûÑÎ≤†Îî© Ïú†ÏÇ¨ÎèÑ)"""
    try:
        # ÏûÑÎ≤†Îî©Ïù¥ ÏûàÎäî ÎπÑÎîîÏò§Îßå Í≤ÄÏÉâ
        embedding_query = """
            SELECT 
                v.video_yid as id,
                v.title,
                v.description,
                v.published_at,
                c.title as channel_name,
                (v.statistics->>'view_count')::int as view_count,
                (v.statistics->>'like_count')::int as like_count,
                (v.statistics->>'comment_count')::int as comment_count,
                v.tags,
                v.thumbnails,
                v.privacy_status,
                v.license,
                v.embeddable,
                v.made_for_kids,
                v.recording_location,
                v.recording_date,
                v.localizations,
                v.topic_categories,
                v.relevant_topic_ids,
                e.embedding_vector
            FROM yt2.videos v
            JOIN yt2.channels c ON v.channel_id = c.id
            JOIN yt2.embeddings e ON v.id = e.video_id
            WHERE e.embedding_type = 'title'
        """
        
        cur.execute(embedding_query)
        videos_with_embeddings = cur.fetchall()
        
        if not videos_with_embeddings:
            logger.warning("ÏûÑÎ≤†Îî© Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ Í≤ÄÏÉâÏúºÎ°ú fallback")
            return basic_search(cur, search_term, limit, offset)
        
        # ÏøºÎ¶¨ ÏûÑÎ≤†Îî© ÏÉùÏÑ± (Í∞ÑÎã®Ìïú TF-IDF Í∏∞Î∞ò)
        documents = [f"{v['title']} {v['description'] or ''}" for v in videos_with_embeddings]
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        
        try:
            tfidf_matrix = vectorizer.fit_transform(documents)
            query_vector = vectorizer.transform([search_term])
            
            # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
            similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
            
            # Ïú†ÏÇ¨ÎèÑ ÏàúÏúºÎ°ú Ï†ïÎ†¨
            similarity_scores = list(enumerate(similarities))
            similarity_scores.sort(key=lambda x: x[1], reverse=True)
            
            # Í≤∞Í≥º ÌïÑÌÑ∞ÎßÅ Î∞è ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò
            filtered_results = [(idx, score) for idx, score in similarity_scores if score > 0.1]
            paginated_results = filtered_results[offset:offset + limit]
            
            # Í≤∞Í≥º ÎπÑÎîîÏò§ Î∞òÌôò
            result_videos = []
            for idx, score in paginated_results:
                video = videos_with_embeddings[idx]
                result_videos.append(video)
            
            return result_videos, len(filtered_results)
            
        except Exception as e:
            logger.error(f"ÏùòÎØ∏ Í≤ÄÏÉâ ÏûÑÎ≤†Îî© Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            return basic_search(cur, search_term, limit, offset)
        
    except Exception as e:
        logger.error(f"ÏùòÎØ∏ Í∏∞Î∞ò Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# üòä SENTIMENT ANALYSIS SEARCH ALGORITHMS
# =============================================================================

def sentiment_search(cur, search_term: str, limit: int, offset: int) -> tuple:
    """Í∞êÏ†ï Î∂ÑÏÑùÏù¥ Ìè¨Ìï®Îêú Í≤ÄÏÉâ"""
    try:
        # Í∏∞Î≥∏ Í≤ÄÏÉâÏúºÎ°ú ÎπÑÎîîÏò§ Ï∞æÍ∏∞
        videos, total_count = basic_search(cur, search_term, limit * 2, offset)
        
        if not videos:
            return [], 0
        
        # Í∞Å ÎπÑÎîîÏò§Ïùò Í∞êÏ†ï Ï†êÏàò Ï°∞Ìöå
        video_ids = [v['id'] for v in videos]
        placeholders = ",".join(["%s"] * len(video_ids))
        
        sentiment_query = f"""
            SELECT 
                v.video_yid,
                COALESCE(AVG(c.sentiment_score), 0) as avg_sentiment,
                COUNT(c.id) as comment_count
            FROM yt2.videos v
            LEFT JOIN yt2.comments c ON v.id = c.video_id
            WHERE v.video_yid IN ({placeholders})
            GROUP BY v.video_yid
        """
        
        cur.execute(sentiment_query, video_ids)
        sentiment_data = {row['video_yid']: {
            'avg_sentiment': row['avg_sentiment'],
            'comment_count': row['comment_count']
        } for row in cur.fetchall()}
        
        # Í∞êÏ†ï Ï†êÏàòÎ•º Í≥†Î†§Ìïú ÏµúÏ¢Ö Ï†êÏàò Í≥ÑÏÇ∞
        scored_videos = []
        for video in videos:
            video_id = video['id']
            sentiment_info = sentiment_data.get(video_id, {'avg_sentiment': 0, 'comment_count': 0})
            
            # Í∏∞Î≥∏ Í¥ÄÎ†®ÎèÑ Ï†êÏàò (ÏàúÏúÑ Í∏∞Î∞ò)
            base_score = 1.0 - videos.index(video) / len(videos)
            
            # Í∞êÏ†ï Ï†êÏàò Î≥¥ÎÑàÏä§ (Í∏çÏ†ïÏ†Å ÎåìÍ∏ÄÏù¥ ÎßéÏùÄ ÏòÅÏÉÅÏóê Í∞ÄÏ†ê)
            sentiment_bonus = max(0, sentiment_info['avg_sentiment']) * 0.3
            
            # ÎåìÍ∏Ä Ïàò Î≥¥ÎÑàÏä§ (ÎåìÍ∏ÄÏù¥ ÎßéÏùÄ ÏòÅÏÉÅÏóê Í∞ÄÏ†ê)
            comment_bonus = min(0.2, sentiment_info['comment_count'] / 100) * 0.2
            
            final_score = base_score + sentiment_bonus + comment_bonus
            
            scored_videos.append((video, final_score))
        
        # ÏµúÏ¢Ö Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨
        scored_videos.sort(key=lambda x: x[1], reverse=True)
        
        # ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò Ï†ÅÏö©
        final_videos = [video for video, score in scored_videos[:limit]]
        
        return final_videos, len(scored_videos)
        
    except Exception as e:
        logger.error(f"Í∞êÏ†ï Î∂ÑÏÑù Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        return basic_search(cur, search_term, limit, offset)

# =============================================================================
# üéØ SEARCH ALGORITHM ROUTER
# =============================================================================

def execute_search_algorithm(algorithm: str, cur, search_term: str, limit: int, offset: int) -> tuple:
    """Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò Ïã§Ìñâ ÎùºÏö∞ÌÑ∞"""
    algorithm_map = {
        "basic": basic_search,
        "tfidf": tfidf_search,
        "weighted": weighted_search,
        "bm25": opensearch_bm25_search,
        "hybrid": hybrid_search,
        "semantic": semantic_search,
        "sentiment": sentiment_search
    }
    
    search_func = algorithm_map.get(algorithm, basic_search)
    logger.info(f"Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò Ïã§Ìñâ: {algorithm}")
    
    return search_func(cur, search_term, limit, offset)

# =============================================================================
# ü§ñ AI STATISTICS & RECOMMENDATION MODELS
# =============================================================================

class VideoStats(BaseModel):
    video_id: str
    title: str
    channel_name: str
    view_count: int
    like_count: int
    comment_count: int
    published_at: str
    engagement_rate: float
    popularity_score: float

class ChannelStats(BaseModel):
    channel_id: str
    channel_name: str
    video_count: int
    total_views: int
    avg_views: float
    total_likes: int
    avg_likes: float
    engagement_rate: float

class TrendData(BaseModel):
    period: str
    video_count: int
    total_views: int
    avg_views: float
    top_keywords: List[str]

class RecommendationResponse(BaseModel):
    video_id: str
    title: str
    channel_name: str
    thumbnail_url: str
    view_count: int
    like_count: int
    published_at: str
    similarity_score: float
    recommendation_reason: str

# =============================================================================
# üìä STATISTICS FUNCTIONS
# =============================================================================

def get_popular_videos(cur, limit: int = 10) -> List[VideoStats]:
    """Ïù∏Í∏∞ ÎπÑÎîîÏò§ ÌÜµÍ≥Ñ Ï°∞Ìöå"""
    query = """
    SELECT 
        v.video_yid as video_id,
        v.title,
        c.title as channel_name,
        (v.statistics->>'view_count')::int as view_count,
        (v.statistics->>'like_count')::int as like_count,
        (v.statistics->>'comment_count')::int as comment_count,
        v.published_at,
        CASE 
            WHEN (v.statistics->>'view_count')::int > 0 
            THEN ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int * 100
            ELSE 0 
        END as engagement_rate,
        CASE 
            WHEN (v.statistics->>'view_count')::int > 0 
            THEN LOG((v.statistics->>'view_count')::int + 1) * 
                 (1 + ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int)
            ELSE 0 
        END as popularity_score
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.statistics->>'view_count' IS NOT NULL
    ORDER BY popularity_score DESC
    LIMIT %s
    """
    
    cur.execute(query, (limit,))
    results = cur.fetchall()
    
    return [
        VideoStats(
            video_id=row[0],
            title=row[1],
            channel_name=row[2],
            view_count=row[3] or 0,
            like_count=row[4] or 0,
            comment_count=row[5] or 0,
            published_at=row[6].isoformat() if row[6] else "",
            engagement_rate=round(row[7], 2),
            popularity_score=round(row[8], 2)
        )
        for row in results
    ]

def get_channel_stats(cur) -> List[ChannelStats]:
    """Ï±ÑÎÑêÎ≥Ñ ÌÜµÍ≥Ñ Ï°∞Ìöå"""
    query = """
    SELECT 
        c.id as channel_id,
        c.title as channel_name,
        COUNT(v.id) as video_count,
        SUM((v.statistics->>'view_count')::int) as total_views,
        AVG((v.statistics->>'view_count')::int) as avg_views,
        SUM((v.statistics->>'like_count')::int) as total_likes,
        AVG((v.statistics->>'like_count')::int) as avg_likes,
        AVG(
            CASE 
                WHEN (v.statistics->>'view_count')::int > 0 
                THEN ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int * 100
                ELSE 0 
            END
        ) as engagement_rate
    FROM yt2.channels c
    LEFT JOIN yt2.videos v ON c.id = v.channel_id
    GROUP BY c.id, c.title
    ORDER BY total_views DESC
    """
    
    cur.execute(query)
    results = cur.fetchall()
    
    return [
        ChannelStats(
            channel_id=str(row[0]),
            channel_name=row[1],
            video_count=row[2],
            total_views=row[3] or 0,
            avg_views=round(row[4] or 0, 2),
            total_likes=row[5] or 0,
            avg_likes=round(row[6] or 0, 2),
            engagement_rate=round(row[7] or 0, 2)
        )
        for row in results
    ]

def get_trend_data(cur, period: str = "month") -> List[TrendData]:
    """Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå"""
    if period == "month":
        date_format = "YYYY-MM"
        group_by = "DATE_TRUNC('month', published_at)"
    elif period == "week":
        date_format = "YYYY-\"W\"WW"
        group_by = "DATE_TRUNC('week', published_at)"
    else:  # day
        date_format = "YYYY-MM-DD"
        group_by = "DATE_TRUNC('day', published_at)"
    
    query = f"""
    SELECT 
        TO_CHAR({group_by}, '{date_format}') as period,
        COUNT(*) as video_count,
        SUM((statistics->>'view_count')::int) as total_views,
        AVG((statistics->>'view_count')::int) as avg_views
    FROM yt2.videos
    WHERE published_at IS NOT NULL
    GROUP BY {group_by}
    ORDER BY {group_by} DESC
    LIMIT 12
    """
    
    cur.execute(query)
    results = cur.fetchall()
    
    return [
        TrendData(
            period=row[0],
            video_count=row[1],
            total_views=row[2] or 0,
            avg_views=round(row[3] or 0, 2),
            top_keywords=[]  # TODO: ÌÇ§ÏõåÎìú Î∂ÑÏÑù Ï∂îÍ∞Ä
        )
        for row in results
    ]

# =============================================================================
# üéØ RECOMMENDATION FUNCTIONS
# =============================================================================

def get_content_based_recommendations(cur, video_id: str, limit: int = 5) -> List[RecommendationResponse]:
    """ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú (Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î∞©Ïãù)"""
    # 1. Í∏∞Ï§Ä ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Ï°∞Ìöå
    base_query = """
    SELECT title, description, tags
    FROM yt2.videos
    WHERE video_yid = %s
    """
    cur.execute(base_query, (video_id,))
    base_video = cur.fetchone()
    
    if not base_video:
        return []
    
    # 2. Î™®Îì† ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Ï°∞Ìöå
    all_query = """
    SELECT v.video_yid, v.title, v.description, v.tags, c.title as channel_name,
           v.statistics->>'view_count' as view_count,
           v.statistics->>'like_count' as like_count,
           v.published_at,
           v.thumbnails->'default'->>'url' as thumbnail_url
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.video_yid != %s
    """
    cur.execute(all_query, (video_id,))
    all_videos = cur.fetchall()
    
    if not all_videos:
        return []
    
    # 3. TF-IDF Î≤°ÌÑ∞Ìôî
    base_text = f"{base_video[0]} {base_video[1]} {' '.join(base_video[2] or [])}"
    all_texts = [f"{row[1]} {row[2]} {' '.join(row[3] or [])}" for row in all_videos]
    
    vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform(all_texts)
    base_vector = vectorizer.transform([base_text])
    
    # 4. Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
    similarities = cosine_similarity(base_vector, tfidf_matrix).flatten()
    
    # 5. ÏÉÅÏúÑ Í≤∞Í≥º ÏÑ†ÌÉù
    top_indices = similarities.argsort()[-limit:][::-1]
    
    recommendations = []
    for idx in top_indices:
        if similarities[idx] > 0.1:  # ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï
            video = all_videos[idx]
            recommendations.append(RecommendationResponse(
                video_id=video[0],
                title=video[1],
                channel_name=video[4],
                thumbnail_url=video[8] or "",
                view_count=int(video[5] or 0),
                like_count=int(video[6] or 0),
                published_at=video[7].isoformat() if video[7] else "",
                similarity_score=round(similarities[idx], 3),
                recommendation_reason="Ï†úÎ™©Í≥º ÏÑ§Î™ÖÏù¥ Ïú†ÏÇ¨Ìï©ÎãàÎã§"
            ))
    
    return recommendations

def get_youtube_video_info(video_id: str) -> dict:
    """YouTube APIÎ•º ÌÜµÌï¥ ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Ï°∞Ìöå"""
    try:
        from googleapiclient.discovery import build
        
        # YouTube API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±
        youtube = build('youtube', 'v3', developerKey=os.getenv('YOUTUBE_API_KEY'))
        
        # ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Ï°∞Ìöå
        request = youtube.videos().list(
            part='snippet,statistics',
            id=video_id
        )
        response = request.execute()
        
        if not response['items']:
            return None
            
        video = response['items'][0]
        snippet = video['snippet']
        statistics = video['statistics']
        
        return {
            'video_id': video_id,
            'title': snippet['title'],
            'description': snippet['description'],
            'channel_title': snippet['channelTitle'],
            'published_at': snippet['publishedAt'],
            'thumbnails': snippet['thumbnails'],
            'view_count': int(statistics.get('viewCount', 0)),
            'like_count': int(statistics.get('likeCount', 0)),
            'comment_count': int(statistics.get('commentCount', 0)),
            'tags': snippet.get('tags', [])
        }
        
    except Exception as e:
        logger.error(f"YouTube API Ïò§Î•ò: {e}")
        if "quotaExceeded" in str(e) or "403" in str(e):
            raise HTTPException(status_code=429, detail="YouTube API Ìï†ÎãπÎüâÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.")
        else:
            raise HTTPException(status_code=500, detail=f"YouTube API Ïò§Î•ò: {str(e)}")

def get_content_based_recommendations_with_youtube_api(cur, video_id: str, limit: int = 5) -> List[RecommendationResponse]:
    """YouTube APIÎ•º ÌôúÏö©Ìïú ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú"""
    try:
        # 1. YouTube APIÎ°ú ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Ï°∞Ìöå
        youtube_video = get_youtube_video_info(video_id)
        if not youtube_video:
            raise HTTPException(status_code=404, detail="Ìï¥Îãπ YouTube ÎπÑÎîîÏò§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
        
        # 2. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïùò Î™®Îì† ÎπÑÎîîÏò§ Ï†ïÎ≥¥ Ï°∞Ìöå
        all_query = """
        SELECT v.video_yid, v.title, v.description, v.tags, c.title as channel_name,
               v.statistics->>'view_count' as view_count,
               v.statistics->>'like_count' as like_count,
               v.published_at,
               v.thumbnails->'default'->>'url' as thumbnail_url
        FROM yt2.videos v
        JOIN yt2.channels c ON v.channel_id = c.id
        """
        cur.execute(all_query)
        all_videos = cur.fetchall()
        
        if not all_videos:
            return []
        
        # 3. YouTube ÎπÑÎîîÏò§ÏôÄ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÎπÑÎîîÏò§Îì§ÏùÑ TF-IDFÎ°ú ÎπÑÍµê
        youtube_text = f"{youtube_video['title']} {youtube_video['description']} {' '.join(youtube_video['tags'])}"
        db_texts = [f"{row[1]} {row[2]} {' '.join(row[3] or [])}" for row in all_videos]
        
        # 4. TF-IDF Î≤°ÌÑ∞Ìôî
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        all_texts = [youtube_text] + db_texts
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        # 5. Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        youtube_vector = tfidf_matrix[0:1]
        db_vectors = tfidf_matrix[1:]
        similarities = cosine_similarity(youtube_vector, db_vectors).flatten()
        
        # 6. ÏÉÅÏúÑ Í≤∞Í≥º ÏÑ†ÌÉù
        top_indices = similarities.argsort()[-limit:][::-1]
        
        recommendations = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï
                video = all_videos[idx]
                recommendations.append(RecommendationResponse(
                    video_id=video[0],
                    title=video[1],
                    channel_name=video[4],
                    thumbnail_url=video[8] or "",
                    view_count=int(video[5] or 0),
                    like_count=int(video[6] or 0),
                    published_at=video[7].isoformat() if video[7] else "",
                    similarity_score=round(similarities[idx], 3),
                    recommendation_reason=f"'{youtube_video['title']}'ÏôÄ Ïú†ÏÇ¨Ìïú ÏΩòÌÖêÏ∏†ÏûÖÎãàÎã§"
                ))
        
        return recommendations
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú Ïò§Î•ò: {e}")
        raise HTTPException(status_code=500, detail=f"Ï∂îÏ≤ú ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}")

def get_popularity_based_recommendations(cur, limit: int = 5) -> List[RecommendationResponse]:
    """Ïù∏Í∏∞ÎèÑ Í∏∞Î∞ò Ï∂îÏ≤ú"""
    query = """
    SELECT 
        v.video_yid,
        v.title,
        c.title as channel_name,
        v.statistics->>'view_count' as view_count,
        v.statistics->>'like_count' as like_count,
        v.published_at,
        v.thumbnails->'default'->>'url' as thumbnail_url,
        CASE 
            WHEN (v.statistics->>'view_count')::int > 0 
            THEN LOG((v.statistics->>'view_count')::int + 1) * 
                 (1 + ((v.statistics->>'like_count')::int + (v.statistics->>'comment_count')::int)::float / (v.statistics->>'view_count')::int)
            ELSE 0 
        END as popularity_score
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.statistics->>'view_count' IS NOT NULL
    ORDER BY popularity_score DESC
    LIMIT %s
    """
    
    cur.execute(query, (limit,))
    results = cur.fetchall()
    
    return [
        RecommendationResponse(
            video_id=row[0],
            title=row[1],
            channel_name=row[2],
            thumbnail_url=row[6] or "",
            view_count=int(row[3] or 0),
            like_count=int(row[4] or 0),
            published_at=row[5].isoformat() if row[5] else "",
            similarity_score=round(row[7], 3),
            recommendation_reason="ÎÜíÏùÄ Ïù∏Í∏∞ÎèÑÎ•º Î≥¥ÏûÖÎãàÎã§"
        )
        for row in results
    ]

def get_trending_recommendations(cur, limit: int = 5) -> List[RecommendationResponse]:
    """ÏµúÏã† Ìä∏Î†åÎìú Ï∂îÏ≤ú"""
    query = """
    SELECT 
        v.video_yid,
        v.title,
        c.title as channel_name,
        v.statistics->>'view_count' as view_count,
        v.statistics->>'like_count' as like_count,
        v.published_at,
        v.thumbnails->'default'->>'url' as thumbnail_url,
        EXTRACT(EPOCH FROM (NOW() - v.published_at)) / 86400 as days_ago
    FROM yt2.videos v
    JOIN yt2.channels c ON v.channel_id = c.id
    WHERE v.published_at IS NOT NULL
    ORDER BY v.published_at DESC
    LIMIT %s
    """
    
    cur.execute(query, (limit,))
    results = cur.fetchall()
    
    return [
        RecommendationResponse(
            video_id=row[0],
            title=row[1],
            channel_name=row[2],
            thumbnail_url=row[6] or "",
            view_count=int(row[3] or 0),
            like_count=int(row[4] or 0),
            published_at=row[5].isoformat() if row[5] else "",
            similarity_score=round(1.0 / (1.0 + float(row[7] or 0)), 3),  # decimal.DecimalÏùÑ floatÎ°ú Î≥ÄÌôò
            recommendation_reason="ÏµúÏã† ÏΩòÌÖêÏ∏†ÏûÖÎãàÎã§"
        )
        for row in results
    ]

# =============================================================================
# üåê API ENDPOINTS
# =============================================================================
@app.get("/")
async def root():
    """Î£®Ìä∏ ÏóîÎìúÌè¨Ïù∏Ìä∏"""
    return {
        "message": "YT2 API ÏÑúÎ≤Ñ",
        "description": "ÏàòÏõêÏãú ÌñâÍ∂ÅÎèô YouTube Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÉâ API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
    try:
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÌôïÏù∏
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
        
        # OpenSearch Ïó∞Í≤∞ ÌôïÏù∏
        OS_CLIENT.ping()
        
        # Redis Ïó∞Í≤∞ ÌôïÏù∏
        REDIS_CLIENT.ping()
        
        return {
            "status": "healthy",
            "database": "connected",
            "opensearch": "connected",
            "redis": "connected",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Ìó¨Ïä§ Ï≤¥ÌÅ¨ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÏÑúÎπÑÏä§ ÏÉÅÌÉú Î∂àÎüâ: {str(e)}")

@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÜµÍ≥Ñ"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM yt2.channels) as total_channels,
                        (SELECT COUNT(*) FROM yt2.videos) as total_videos,
                        (SELECT COUNT(*) FROM yt2.comments) as total_comments,
                        (SELECT COUNT(*) FROM yt2.embeddings) as total_embeddings,
                        (SELECT COUNT(*) FROM yt2.videos WHERE created_at >= NOW() - INTERVAL '24 hours') as videos_last_24h,
                        (SELECT COUNT(*) FROM yt2.videos WHERE created_at >= NOW() - INTERVAL '7 days') as videos_last_7d
                """)
                
                stats = cur.fetchone()
                
                return StatsResponse(
                    total_channels=stats[0],
                    total_videos=stats[1],
                    total_comments=stats[2],
                    total_embeddings=stats[3],
                    videos_last_24h=stats[4],
                    videos_last_7d=stats[5]
                )
    except Exception as e:
        logger.error(f"ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/api/search", response_model=SearchResponse)
async def search_videos(
    q: str = Query(..., description="Í≤ÄÏÉâÏñ¥"),
    limit: int = Query(10, ge=1, le=100, description="Í≤∞Í≥º Ïàò Ï†úÌïú"),
    page: int = Query(1, ge=1, description="ÌéòÏù¥ÏßÄ Î≤àÌò∏"),
    algorithm: str = Query("basic", description="Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò"),
    offset: int = Query(0, ge=0, description="Í≤∞Í≥º Ïò§ÌîÑÏÖã")
):
    """ÏòÅÏÉÅ Í≤ÄÏÉâ"""
    start_time = datetime.now()
    
    # ÌéòÏù¥ÏßÄ Í∏∞Î∞ò Ïò§ÌîÑÏÖã Í≥ÑÏÇ∞
    actual_offset = (page - 1) * limit if page > 0 else offset
    
    try:
        # Ï∫êÏãú ÌôïÏù∏
        cache_key = f"search:{q}:{limit}:{page}:{algorithm}"
        cached_result = REDIS_CLIENT.get(cache_key)
        if cached_result:
            logger.info(f"Ï∫êÏãúÏóêÏÑú Í≤∞Í≥º Î∞òÌôò: {q} (ÏïåÍ≥†Î¶¨Ï¶ò: {algorithm})")
            return json.loads(cached_result)
        
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                search_term = f"%{q}%"
                
                # üéØ Í≤ÄÏÉâ ÏïåÍ≥†Î¶¨Ï¶ò Ïã§Ìñâ
                videos, total_count = execute_search_algorithm(algorithm, cur, search_term, limit, actual_offset)
                
                # Í≤∞Í≥º Î≥ÄÌôò
                video_responses = []
                for video in videos:
                    video_responses.append(VideoResponse(
                        id=video['id'],
                        title=video['title'],
                        description=video['description'],
                        published_at=video['published_at'].isoformat() if video['published_at'] else None,
                        channel_name=video['channel_name'],
                        view_count=video['view_count'] or 0,
                        like_count=video['like_count'] or 0,
                        comment_count=video['comment_count'] or 0,
                        tags=video['tags'] or [],
                        thumbnails=video['thumbnails'] or {},
                        # Ï∂îÍ∞ÄÎêú ÌïÑÎìúÎì§
                        privacy_status=video.get('privacy_status'),
                        license=video.get('license'),
                        embeddable=video.get('embeddable'),
                        made_for_kids=video.get('made_for_kids'),
                        recording_location=video.get('recording_location'),
                        recording_date=video['recording_date'].isoformat() if video.get('recording_date') else None,
                        localizations=video.get('localizations'),
                        topic_categories=video.get('topic_categories') or [],
                        relevant_topic_ids=video.get('relevant_topic_ids') or []
                    ))
                
                search_time = (datetime.now() - start_time).total_seconds()
                total_pages = (total_count + limit - 1) // limit  # Ïò¨Î¶º Í≥ÑÏÇ∞
                
                result = SearchResponse(
                    videos=video_responses,
                    total_count=total_count,
                    total_pages=total_pages,
                    query=q,
                    search_time=search_time
                )
                
                # Ï∫êÏãú Ï†ÄÏû• (5Î∂Ñ)
                REDIS_CLIENT.setex(cache_key, 300, json.dumps(result.dict(), default=str))
                
                # Í≤ÄÏÉâ Î°úÍ∑∏ Ï†ÄÏû•
                log_search(q, len(video_responses), search_time)
                
                return result
                
    except Exception as e:
        logger.error(f"Í≤ÄÏÉâ Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}")

@app.get("/videos/{video_id}")
async def get_video_detail(video_id: str):
    """ÏòÅÏÉÅ ÏÉÅÏÑ∏ Ï†ïÎ≥¥"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        v.video_yid as id,
                        v.title,
                        v.description,
                        v.published_at,
                        v.duration,
                        v.statistics,
                        v.tags,
                        v.thumbnails,
                        c.title as channel_name,
                        c.description as channel_description,
                        c.statistics as channel_statistics
                    FROM yt2.videos v
                    JOIN yt2.channels c ON v.channel_id = c.id
                    WHERE v.video_yid = %s
                """, (video_id,))
                
                video = cur.fetchone()
                if not video:
                    raise HTTPException(status_code=404, detail="ÏòÅÏÉÅÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
                
                return {
                    "id": video['id'],
                    "title": video['title'],
                    "description": video['description'],
                    "published_at": video['published_at'].isoformat() if video['published_at'] else None,
                    "duration": video['duration'],
                    "statistics": video['statistics'],
                    "tags": video['tags'],
                    "thumbnails": video['thumbnails'],
                    "channel": {
                        "name": video['channel_name'],
                        "description": video['channel_description'],
                        "statistics": video['channel_statistics']
                    }
                }
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ÏòÅÏÉÅ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÏòÅÏÉÅ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/channels")
async def get_channels(
    limit: int = Query(10, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """Ï±ÑÎÑê Î™©Î°ù"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        c.channel_yid as id,
                        c.title,
                        c.description,
                        c.statistics,
                        c.thumbnails,
                        COUNT(v.id) as video_count
                    FROM yt2.channels c
                    LEFT JOIN yt2.videos v ON c.id = v.channel_id
                    GROUP BY c.id, c.channel_yid, c.title, c.description, c.statistics, c.thumbnails
                    ORDER BY video_count DESC
                    LIMIT %s OFFSET %s
                """, (limit, offset))
                
                channels = cur.fetchall()
                
                return [
                    {
                        "id": channel['id'],
                        "title": channel['title'],
                        "description": channel['description'],
                        "statistics": channel['statistics'],
                        "thumbnails": channel['thumbnails'],
                        "video_count": channel['video_count']
                    }
                    for channel in channels
                ]
                
    except Exception as e:
        logger.error(f"Ï±ÑÎÑê Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ï±ÑÎÑê Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/playlists")
async def get_playlists(
    channel_id: Optional[str] = Query(None, description="Ï±ÑÎÑê ID"),
    limit: int = Query(10, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """Ïû¨ÏÉùÎ™©Î°ù Î™©Î°ù"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                if channel_id:
                    cur.execute("""
                        SELECT 
                            p.playlist_yid as id,
                            p.title,
                            p.description,
                            p.thumbnails,
                            p.item_count,
                            p.privacy_status,
                            p.localizations,
                            c.title as channel_name
                        FROM yt2.playlists p
                        JOIN yt2.channels c ON p.channel_id = c.id
                        WHERE c.channel_yid = %s
                        ORDER BY p.created_at DESC
                        LIMIT %s OFFSET %s
                    """, (channel_id, limit, offset))
                else:
                    cur.execute("""
                        SELECT 
                            p.playlist_yid as id,
                            p.title,
                            p.description,
                            p.thumbnails,
                            p.item_count,
                            p.privacy_status,
                            p.localizations,
                            c.title as channel_name
                        FROM yt2.playlists p
                        JOIN yt2.channels c ON p.channel_id = c.id
                        ORDER BY p.created_at DESC
                        LIMIT %s OFFSET %s
                    """, (limit, offset))
                
                playlists = cur.fetchall()
                
                return [
                    {
                        "id": playlist['id'],
                        "title": playlist['title'],
                        "description": playlist['description'],
                        "thumbnails": playlist['thumbnails'],
                        "item_count": playlist['item_count'],
                        "privacy_status": playlist['privacy_status'],
                        "localizations": playlist['localizations'],
                        "channel_name": playlist['channel_name']
                    }
                    for playlist in playlists
                ]
                
    except Exception as e:
        logger.error(f"Ïû¨ÏÉùÎ™©Î°ù Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ïû¨ÏÉùÎ™©Î°ù Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/playlists/{playlist_id}/items")
async def get_playlist_items(
    playlist_id: str,
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """Ïû¨ÏÉùÎ™©Î°ù ÏïÑÏù¥ÌÖú Î™©Î°ù"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        pi.playlist_item_yid as id,
                        pi.position,
                        pi.title,
                        pi.description,
                        pi.thumbnails,
                        pi.published_at,
                        v.video_yid as video_id,
                        v.title as video_title,
                        v.duration,
                        v.statistics
                    FROM yt2.playlist_items pi
                    LEFT JOIN yt2.videos v ON pi.video_id = v.id
                    WHERE pi.playlist_id = (
                        SELECT id FROM yt2.playlists WHERE playlist_yid = %s
                    )
                    ORDER BY pi.position
                    LIMIT %s OFFSET %s
                """, (playlist_id, limit, offset))
                
                items = cur.fetchall()
                
                return [
                    {
                        "id": item['id'],
                        "position": item['position'],
                        "title": item['title'],
                        "description": item['description'],
                        "thumbnails": item['thumbnails'],
                        "published_at": item['published_at'].isoformat() if item['published_at'] else None,
                        "video_id": item['video_id'],
                        "video_title": item['video_title'],
                        "duration": item['duration'],
                        "statistics": item['statistics']
                    }
                    for item in items
                ]
                
    except Exception as e:
        logger.error(f"Ïû¨ÏÉùÎ™©Î°ù ÏïÑÏù¥ÌÖú Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ïû¨ÏÉùÎ™©Î°ù ÏïÑÏù¥ÌÖú Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/videos/{video_id}/captions")
async def get_video_captions(video_id: str):
    """ÏòÅÏÉÅ ÏûêÎßâ Î™©Î°ù"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        caption_yid as id,
                        language,
                        name,
                        status,
                        track_kind,
                        is_auto_synced,
                        is_cc,
                        is_draft,
                        is_served,
                        is_auto_generated,
                        last_updated
                    FROM yt2.captions
                    WHERE video_id = (
                        SELECT id FROM yt2.videos WHERE video_yid = %s
                    )
                    ORDER BY language
                """, (video_id,))
                
                captions = cur.fetchall()
                
                return [
                    {
                        "id": caption['id'],
                        "language": caption['language'],
                        "name": caption['name'],
                        "status": caption['status'],
                        "track_kind": caption['track_kind'],
                        "is_auto_synced": caption['is_auto_synced'],
                        "is_cc": caption['is_cc'],
                        "is_draft": caption['is_draft'],
                        "is_served": caption['is_served'],
                        "is_auto_generated": caption['is_auto_generated'],
                        "last_updated": caption['last_updated'].isoformat() if caption['last_updated'] else None
                    }
                    for caption in captions
                ]
                
    except Exception as e:
        logger.error(f"ÏòÅÏÉÅ ÏûêÎßâ Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÏòÅÏÉÅ ÏûêÎßâ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/categories")
async def get_video_categories():
    """ÏòÅÏÉÅ Ïπ¥ÌÖåÍ≥†Î¶¨ Î™©Î°ù"""
    try:
        with get_db_connection() as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT 
                        category_yid as id,
                        title,
                        assignable,
                        channel_id
                    FROM yt2.video_categories
                    ORDER BY category_yid
                """)
                
                categories = cur.fetchall()
                
                return [
                    {
                        "id": category['id'],
                        "title": category['title'],
                        "assignable": category['assignable'],
                        "channel_id": category['channel_id']
                    }
                    for category in categories
                ]
                
    except Exception as e:
        logger.error(f"ÏòÅÏÉÅ Ïπ¥ÌÖåÍ≥†Î¶¨ Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÏòÅÏÉÅ Ïπ¥ÌÖåÍ≥†Î¶¨ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

def log_search(query: str, result_count: int, search_time: float):
    """Í≤ÄÏÉâ Î°úÍ∑∏ Ï†ÄÏû•"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO yt2.search_logs (query, results_count, response_time_ms)
                    VALUES (%s, %s, %s)
                """, (query, result_count, int(search_time * 1000)))
                conn.commit()
    except Exception as e:
        logger.error(f"Í≤ÄÏÉâ Î°úÍ∑∏ Ï†ÄÏû• Ïã§Ìå®: {e}")

# =============================================================================
# ü§ñ AI STATISTICS & RECOMMENDATION API ENDPOINTS
# =============================================================================

@app.get("/api/stats/popular-videos", response_model=List[VideoStats])
async def get_popular_videos_api(limit: int = Query(10, ge=1, le=50, description="Í≤∞Í≥º Ïàò Ï†úÌïú")):
    """Ïù∏Í∏∞ ÎπÑÎîîÏò§ ÌÜµÍ≥Ñ Ï°∞Ìöå"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_popular_videos(cur, limit)
    except Exception as e:
        logger.error(f"Ïù∏Í∏∞ ÎπÑÎîîÏò§ Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ïù∏Í∏∞ ÎπÑÎîîÏò§ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/api/stats/channels", response_model=List[ChannelStats])
async def get_channel_stats_api():
    """Ï±ÑÎÑêÎ≥Ñ ÌÜµÍ≥Ñ Ï°∞Ìöå"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_channel_stats(cur)
    except Exception as e:
        logger.error(f"Ï±ÑÎÑê ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ï±ÑÎÑê ÌÜµÍ≥Ñ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/api/stats/trends", response_model=List[TrendData])
async def get_trend_data_api(period: str = Query("month", description="Í∏∞Í∞Ñ (day, week, month)")):
    """Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_trend_data(cur, period)
    except Exception as e:
        logger.error(f"Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

@app.get("/api/recommendations/content-based", response_model=List[RecommendationResponse])
async def get_content_based_recommendations_api(
    video_id: str = Query(..., description="Í∏∞Ï§Ä ÎπÑÎîîÏò§ ID"),
    limit: int = Query(5, ge=1, le=20, description="Ï∂îÏ≤ú Ïàò Ï†úÌïú")
):
    """ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú (Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î∞©Ïãù)"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_content_based_recommendations(cur, video_id, limit)
    except Exception as e:
        logger.error(f"ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú Ïã§Ìå®: {str(e)}")

@app.get("/api/recommendations/content-based-youtube", response_model=List[RecommendationResponse])
async def get_content_based_recommendations_youtube_api(
    video_id: str = Query(..., description="YouTube ÎπÑÎîîÏò§ ID"),
    limit: int = Query(5, ge=1, le=20, description="Ï∂îÏ≤ú Ïàò Ï†úÌïú")
):
    """YouTube APIÎ•º ÌôúÏö©Ìïú ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_content_based_recommendations_with_youtube_api(cur, video_id, limit)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"YouTube API ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"YouTube API ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú Ïã§Ìå®: {str(e)}")

@app.get("/api/recommendations/popularity", response_model=List[RecommendationResponse])
async def get_popularity_recommendations_api(
    limit: int = Query(5, ge=1, le=20, description="Ï∂îÏ≤ú Ïàò Ï†úÌïú")
):
    """Ïù∏Í∏∞ÎèÑ Í∏∞Î∞ò Ï∂îÏ≤ú"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_popularity_based_recommendations(cur, limit)
    except Exception as e:
        logger.error(f"Ïù∏Í∏∞ÎèÑ Í∏∞Î∞ò Ï∂îÏ≤ú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ïù∏Í∏∞ÎèÑ Í∏∞Î∞ò Ï∂îÏ≤ú Ïã§Ìå®: {str(e)}")

@app.get("/api/recommendations/trending", response_model=List[RecommendationResponse])
async def get_trending_recommendations_api(
    limit: int = Query(5, ge=1, le=20, description="Ï∂îÏ≤ú Ïàò Ï†úÌïú")
):
    """ÏµúÏã† Ìä∏Î†åÎìú Ï∂îÏ≤ú"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                return get_trending_recommendations(cur, limit)
    except Exception as e:
        logger.error(f"Ìä∏Î†åÎìú Ï∂îÏ≤ú Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"Ìä∏Î†åÎìú Ï∂îÏ≤ú Ïã§Ìå®: {str(e)}")

@app.get("/api/stats/overview")
async def get_stats_overview():
    """ÌÜµÍ≥Ñ Í∞úÏöî Ï°∞Ìöå"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ
                cur.execute("""
                    SELECT 
                        COUNT(*) as total_videos,
                        COUNT(DISTINCT channel_id) as total_channels,
                        SUM((statistics->>'view_count')::int) as total_views,
                        AVG((statistics->>'view_count')::int) as avg_views,
                        SUM((statistics->>'like_count')::int) as total_likes,
                        AVG((statistics->>'like_count')::int) as avg_likes
                    FROM yt2.videos
                    WHERE statistics->>'view_count' IS NOT NULL
                """)
                overall_stats = cur.fetchone()
                
                # ÏµúÍ∑º 7Ïùº ÌÜµÍ≥Ñ
                cur.execute("""
                    SELECT 
                        COUNT(*) as recent_videos,
                        SUM((statistics->>'view_count')::int) as recent_views
                    FROM yt2.videos
                    WHERE published_at >= NOW() - INTERVAL '7 days'
                    AND statistics->>'view_count' IS NOT NULL
                """)
                recent_stats = cur.fetchone()
                
                return {
                    "overall": {
                        "total_videos": overall_stats[0] or 0,
                        "total_channels": overall_stats[1] or 0,
                        "total_views": overall_stats[2] or 0,
                        "avg_views": round(overall_stats[3] or 0, 2),
                        "total_likes": overall_stats[4] or 0,
                        "avg_likes": round(overall_stats[5] or 0, 2)
                    },
                    "recent_7_days": {
                        "new_videos": recent_stats[0] or 0,
                        "new_views": recent_stats[1] or 0
                    }
                }
    except Exception as e:
        logger.error(f"ÌÜµÍ≥Ñ Í∞úÏöî Ï°∞Ìöå Ïã§Ìå®: {e}")
        raise HTTPException(status_code=500, detail=f"ÌÜµÍ≥Ñ Í∞úÏöî Ï°∞Ìöå Ïã§Ìå®: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
